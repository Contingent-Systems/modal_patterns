%\documentclass[a4paper,UKenglish]{lipics-v2016}
% For two-column layout for PPoPP, change 'acmsmall' to 'sigplan'.  Probably also turn on the
% 'review' option, and the 'anonymous' option if it's DBR
\documentclass[runningheads]{llncs}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
% for section-numbered lemmas etc., use "numberwithinsect"
\usepackage[export]{adjustbox}

\usepackage{booktabs}
\usepackage{multirow}
 \usepackage{proof}
\usepackage{array}
\newcolumntype{O}[1]{>{\raggedright\arraybackslash}m{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{Q}[1]{>{\raggedleft\arraybackslash}m{#1}}
\newcolumntype{S}{>{\centering\arraybackslash}m{2cm}}

\usepackage{microtype}%if unwanted, comment out or use option "draft"
%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory
%\usepackage{t1enc}
%\usepackage[strings]{underscore}
%%%%%\usepackage{geometry}
\usepackage{wrapfig}
\usepackage[rawfloats=true]{floatrow} 
\restylefloat{figure}    
\DeclareMathSizes{4}{4}{4}{4}
\usepackage{varwidth}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage[fleqn]{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{proof}
\usepackage{stmaryrd}
\usepackage{xspace}
\usepackage{listings}
\usepackage{comment}
\usepackage{longtable}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage[toc,page]{appendix}
\usepackage[export]{adjustbox}
\usepackage{amssymb}
\usepackage{graphics,graphicx}
\usepackage{mathpartir}
\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows.meta,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{calc, shapes, backgrounds}
\usepackage{verbatim}
\usepackage{tikz-qtree}
\usepackage{pgfplots}
\usepackage{caption,capt-of}
\usepackage{color}
\usepackage{soul}
\setul{1ex}{0.8ex}
\definecolor{orange}{rgb}{1,0.5,0}
\setulcolor{orange}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{afterpage}
\usepackage{fontawesome}
\usepackage[utf8]{inputenc}

%%sequeezeeee
\addtolength{\textfloatsep}{-10pt}
\addtolength{\dbltextfloatsep}{-10pt}

\newcommand{\ptree}{\text{\faSitemap}@[\ell]}
\newcommand{\mtree}{\varupphi\text{\faSitemap}@[\ell]^{\Pi}_{\pi}(\sigma)}
\newcommand{\oprtree}[4]{#3\;\text{\faSitemap}@[\textsf{#1_{#2}}]^{\Pi}_{\pi}(#4)}

\newcommand{\osubtree}[4]{#2\;\text{\faSitemap}@[\textsf{node}_{#1}]^{\Pi}_{\pi}(#3_#4)}
\newcommand{\otree}[3]{#2\;\text{\faSitemap}@[\textsf{node}_{#1}]^{\Pi}_{\pi}(#3)}
\newcommand{\oltree}[3]{#2\;\text{\faSitemap}@[#1]^{\Pi}_{\pi}(#3)}

\newcommand{\defeq}{\mathbin{\stackrel{\text{def}}{=}}}
% some stuff for prettier proof outlines
\newcommand{\havoc}{\textsf{havoc}}
\newcommand{\specline}[1]{{\color{blue}\left\{#1\right\}}}
\newcommand{\assertion}[1]{\textsf{assert} {\color{red}\left\{#1\right\}}}
\newcommand{\proofstepnr}[5][1pt/3pt]{\small \left.
\begin{array}{l}
\hdashline[#1]
\ensuremath{\specline{#2}}\\
\begin{array}{l}
#3
\end{array}\\

\ensuremath{\specline{#5}}\\
\hdashline[1pt/3pt]
\end{array}
\color{green}\right)\hspace{-6pt}{\color{green}-}
\begin{array}{@{}l@{}}
#4
\end{array}
}
\newcommand{\proofoutstepnr}[5][1pt/3pt]{\small \left.
\begin{array}{@{}l@{}}
\hdashline[#1]
\ensuremath{\specline{#2}}\\
\begin{array}{@{\;\;\;}l}
#3
\end{array}\\
\ensuremath{\specline{#5}}\\
\hdashline[1pt/3pt]
\end{array}
\color{green}\right)\hspace{-6pt}{\color{green}-}
\begin{array}{@{}l@{}}
#4
\end{array}
}
% \proofoutstep[dash stuff]{pre}{C}{rule}{post}
\newcommand{\proofoutstep}[5][1pt/3pt]{
\proofoutstepnr[#1]{#2}
{#3}
{\rotatebox[origin=c]{90}{$\begin{array}{@{}c@{}}{}#4\end{array}$}}
{#5}
}
\newcommand{\sepimp}{\mathrel{-\mkern-6mu*}}
\newcommand{\NULL}{\texttt{NULL}}
\newcommand{\NEW}{\texttt{new}}
\newcommand{\SKIP}{\texttt{skip}}
\newcommand{\RESULT}{\texttt{result}}
\newcommand{\class}{\texttt{class}}
\newcommand{\where}{\texttt{where}}
\newcommand{\readable}{\texttt{readable}}
\newcommand{\writable}{\texttt{writable}}
\newcommand{\isolated}{\texttt{isolated}}
\newcommand{\immutable}{\texttt{immutable}}
\newcommand{\consume}[1]{\texttt{consume}(#1)}
\newcommand{\consumeiso}[1]{\mathsf{RemIso}(#1)}
\newcommand{\INT}{\texttt{int}}
\newcommand{\bool}{\texttt{bool}}
\newcommand{\this}{\texttt{this}}
\newcommand{\immutdecl}[1]{\mathsf{ImmutableDeclared}(#1)}
\newcommand{\IsoOrImm}{\textsf{IsoOrImm}}
\newcommand{\NonWrit}{\textsf{NoWrit}}
\newcommand{\db}[1]{\llbracket#1\rrbracket}
\newcommand{\nil}{\textsf{null}}
\newcommand{\atom}{\textsf{atom} \mapsto \mstatename \mapsto \mstatename}
\newcommand{\loc}{\textsf{Loc}}
\newcommand{\val}{\textsf{Val}}
\newcommand{\mstatename}{\textsf{MState}}
\newcommand{\superset}{\mathcal{P}}
\newcommand{\mstatedecl}{\sigma \in \mstatename \overset{def}{=} s \times h \times l \times rt \times R \times B  \times F }
\newcommand{\stack}{s \in \var \times \tid \rightharpoonup \loc}
\newcommand{\heap}{h \in \loc \times \fname \rightharpoonup \val}
\newcommand{\wlock}{l \in \tid \uplus \textsf{unlocked}}
\newcommand{\rlock}{R \in \superset(\tid)}
\newcommand{\fset}{F : \loc \mapsto \superset(\tid) }
\newcommand{\atomeval}{\db{-}:\atom}
\newcommand{\rcuwrite}{\textsf{RCUWrite} \text{ x.f as y in }\, \{ C \}}
\newcommand{\rcuread}{\textsf{RCURead} \text{ x.f as y in }\, \{ C \}}
  \newcommand{\desugarread}{ \rcuread \overset{\Delta}{=} \texttt{\textsf{ReadBegin}}; y = x.f;\{C'\}  ;\textsf{ReadEnd} }
  \newcommand{\desugarwrite}{\rcuwrite \overset{\Delta}{=} \texttt{\textsf{WriteBegin}} ; y = x.f ; \{C'\}  ;\textsf{WriteEnd}}
    \newcommand{\desugarsync}{\textsf{Sync} \overset{\Delta}{=} \texttt{\textsf{SyncStart};\textsf{SyncStop}}}
  %\newcommand{\boundpath}{\bar f_{i}^{k} = \forall_{k \in \mathbb{N},i \in \fnameenum} \ldotp  \underbrace{f_{i}^{0} \ldots f_{i}^{k}}_\text{$\mid$ $f_{i}^{0}$ \ldots $f_{i}^{k}$ $\mid$ = k} }
\newcommand{\subt}{\prec:}
%typing rules
 \newcommand{\fname}{\textsf{FName}}
\newcommand{\fmname}{\textsf{FMap}}
\newcommand{\uiglobal}{\textsf{UIGlobal}}
\newcommand{\udef}{\textsf{undef}}
\newcommand{\unlinked}{\textsf{unlinked}}
\newcommand{\fresh}{\textsf{fresh}}
\newcommand{\var}{\textsf{Var}}
\newcommand{\tid}{\textsf{TID}}

\newcommand{\fmapfieldemp}{\N_{f,\emptyset} = \N \setminus \{f \rightharpoonup \_ \}}
\newcommand{\fmapname}{\N = \{ f_0|\ldots|f_n\rightharpoonup  \{y\} \mid f_{i} \in \fname \land \; 0\leq i\leq n  \land \; (y \in \var \lor y \in \{null\}) \}}
\newcommand{\fmapextend}{  \N(f\rightharpoonup x \setminus y) = \N \setminus \{f \rightharpoonup x\} \cup \{f \rightharpoonup y\} }
\newcommand{\fmapextendn}{  \N(\cup_{f\rightharpoonup y}) = \N \cup \{f \rightharpoonup y\} }
\newcommand{\fmapdiff}{\N(\setminus_{f\rightharpoonup y}) = \N - \{f\rightharpoonup y\}}
\newcommand{\fmapwithf}{\N([f\rightharpoonup  y]) = \N \text{   where  } f \rightharpoonup y \in \N }
\newcommand{\fmapempty}{\N_{\emptyset} =  \{ \}}
\newcommand{\codom}{\cod(\N)=\dom(\Gamma)}
\newcommand{\cod}{\texttt{cod}}
\newcommand{\dom}{\texttt{dom}}
%\newcommand{\types}{\tau ::= \textsf{rcu} | \textsf{normal}}

\newcommand{\rpth}{f \in \textsf{FName} 
     \left\{\begin{array}{cl}
     \epsilon &    \\
     f  &  f \in \fname\\
   f_1 | \ldots | f_n &\forall_{1 \leq i \leq n} \ldotp f_i \in \fname 
     \end{array}\right\}
     \quad \text{  where } \textsf{FType(f) = \textsf{RCU}}
}
\newcommand{\rextendswith}{\rho^{k}[i] \mapsto f_i \text{  where  } \forall_{0\leq i \leq k} \ldotp f_i \in \fname }
\newcommand{\extendswith}{\rho[i] \mapsto \rho_{i}^{k} \text{  where  } \forall_{0\leq i \leq n} \ldotp \rho_i^{k} \in \textsf{RelativePath}}

\newcommand{\pth}{\rho \in \textsf{Path}  \qquad \rho ::= \rho_1 \ldots\rho_n  \text{   where  } \forall_{0\leq i \leq n}\ldotp \rho_i \in \textsf{RelativePath} \text{ or } \rho_i \in \textsf{ConcretePath}}
\newcommand{\cpath}{\forall_{0 \leq i \leq n} \ldotp f_i \in \mathsf{FName} \land l_i \in \textsf{Label} \land \exists_{k\geq0} \ldotp f_i^{l_i} = \overbrace{f_i\ldots f_i}^{k} \land (\forall_{0\leq j \leq n} \ldotp j\neq i \implies l_i \neq l_j) }



\newcommand{\rpath}{\rho \in \textsf{RelativePath} \quad \rho = (f)^{*} \text{   where  } f \in \textsf{FName} \text{  and  } \exists_{k>=0} \ldotp (f)^{*} = \overbrace{f\ldots f}^\text{k} }
\newcommand{\falt}{f \in \textsf{AlternatingFName} = f_1 | f_2 | \ldots | f_{n-1} | f_n \text{  where  } \forall_{0\leq i \leq n}\ldotp f_i \in \textsf{FName} }

\newcommand{\lenmap}{\mathcal{L} \in \textsf{LPath} : \rho \mapsto \mathbb{N}}
\newcommand{\etadecl}{\eta = \{f \mapsto y \mid f \in \fname \land y\in \var \}  }
\newcommand{\rcunfd}{\textsf{rcuFresh} \,  \N}
\newcommand{\rcunfdecl}{x : \textsf{rcuFresh} \;\; \N }
\newcommand{\rcunf}[3]{#1 : \textsf{rcuFresh} \;\; \N(\cup_{ #2 \rightharpoonup #3}) } %x, f , y
\newcommand{\rcuf}[1]{#1 : \textsf{rcuFresh} \;\;\N } %x, f , y
\newcommand{\rcunextfresh}[1]{\textsf{rcuFresh} \;\; \N #1 }
\newcommand{\ftypedecl}{\textsf{FType} : \fname \mapsto \tau}
\newcommand{\ftype}[2]{\textsf{FType}(f_{#1}) \mapsto \textsf{#2}}
%%%
\newcommand{\typedepthdecl}{\textsf{TypeDepth}(x:\text{T}) =  \mid\rho\mid \;\; \texttt{where T} = \textsf{rcuItr} \underset{y}{\mid} \rho \,\, \N }
%%%
\newcommand{\rcuonly}{\textsf{RCUOnly}(x) \overset{\texttt{def}}{=} x:T \text{ where } T = \textsf{rcuItr}\_}
\newcommand{\rcumodonlydecl}{\textsf{RCUModifiedOnly}(C) \overset{\texttt{def}}{=} \forall_{x \in \textsf{Modifies}(C)}\ldotp \textsf{RcuOnly}(x) }
\newcommand{\rcufilterb}{\textsf{RemRcu} : \Gamma \mapsto \Gamma}
\newcommand{\rcufilterr}[1]{\textsf{RemRcu} ( #1 ) = \texttt{filter} \, ( \lambda x \ldotp x \neq \textsf{rcuItr \_ } ) \,  #1 }
\newcommand{\rcufiltera}[1]{\textsf{RemRcu} ( #1 ) }
%%%Alias
\newcommand{\aliaswith}[2]{#1 \in \textsf{AliasWith}(#2)}
\newcommand{\aliaswithdecl}{\textsf{AliasWith}(x) = \{y \mid y:\textsf{rcuItr}\underset{r',k'}{\mid} \rho' \;\; \N' \land \rho = \rho' \land \N = \N' \}  \texttt{  where  }  \rcuitrT{x}{r}{k}{kk}{\_} }
%%%Framing Type Contexts
    \newcommand{\wellframed}{
      \mathsf{WellFramed}(\Gamma,\Delta)\overset{def}{=}
      \left\{\begin{array}{l}
      
        \forall_{z\in dom(\Gamma),f,\rho,y }\ldotp                           z\notin dom(\Delta) \land            z:\textsf{rcuItr}\,\rho\,\N([f\rightharpoonup y]) \implies \\
                      \left\{ 
	              \begin{array}{l}
                        y\notin dom(\Delta) \land y\in dom(\Gamma) \land \forall_{n\in dom(\Delta),\rho',f',m}\ldotp n\notin dom(\Gamma) \land 	\\
            \left\{
            \begin{array}{l}
              n:\textsf{rcuItr}\,\rho'\,\N([f'\rightharpoonup m]) \implies \\
              \left\{\begin{array}{l}
              m \notin dom(\Gamma) \land \\
              m \in dom(\Delta) \land
              \rho \neq \rho'                    
              \end{array} \right\}
            \end{array}\right\}
     \end{array}\right\}
         \end{array}\right.      
      
}

%%%Framing Type Contexts ends
\newcommand{\deltaepsilon}{\Delta >= 0 \qquad 0\leq \epsilon \leq \Delta }
\newcommand{\reduce}[2]{\textsf{InReduction}(#1,#2) = \{ \Gamma' \mid  \Gamma' \preceq_{\varepsilon} \Gamma \} }
\newcommand{\rename}[2]{\textsf{ContextRename}(#1,#2) = #2 \mapsto #1 }
%%Sep RCU
\newcommand{\seprcu}[4]{\textsf{SepLoop}(#1,#2,#3,#4) \overset{\texttt{def}}{=} \text{dom}(#1)\cap \text{dom}(#2) = \emptyset \, \land \, #1\uplus#2= #4 \, \land \, \textsf{RcuModifiedOnly}(#2,#3)}
\newcommand{\seprcud}[4]{\textsf{SepLoop}(#1,#2,#3,#4)}
%%
\newcommand{\seprcub}[5]{\textsf{SepBranch}(#1,#2,#3,#4,#5) \overset{\texttt{def}}{=}
   \left\{\begin{array}{cl}
  \text{dom}(#2)\cap \text{dom}(#3) = \emptyset \, \land \, #2\uplus#3=#1 \land \\
  \textsf{RcuModifiedOnly}(#2,#4) \,\land\, \textsf{RcuModifiedOnly}(#2,#5)\,\land \\

 \end{array}\right\}
}
\newcommand{\seprcubd}[5]{\textsf{SepBranch}(#1,#2,#3,#4,#5)}
%%
\newcommand{\precgamma}[2]{#1  \preceq #2 \overset{\texttt{def}}{=} \forall_{(x)\in #1}\ldotp x \in #2 \implies  #1 \preceq_{x} #2}
%%%if branches
\newcommand{\iffree}{\textsf{if}(\text{e})\{\text{C}\}}
\newcommand{\ifelse}{\textsf{if}(\text{e})\{\text{C}_1\}\textsf{else}\{\text{C}_2\}}
%%var  root,k  , k+1 mappedvar
\newcommand{\rcuitertype}[4]{\textsf{rcuItr}(#1,#2)\;#3\;#4}
\newcommand{\rcuit}[4]{#1:\textsf{rcuItr}\;\; #3 \;\; #4 }
\newcommand{\rcuitrT}[5]{#1:\textsf{rcuItr}\;\; \rho \;\; \N }
\newcommand{\rcuitrn}[5]{#1:\textsf{rcuItr}\;\; \rho \, f \;\; \N }
\newcommand{\rcuitr}[5]{#1:\textsf{rcuItr}\;\; \rho \;\; \N[f  \rightharpoonup #5] }
\newcommand{\rcustack}[2]{#1 : \textsf{rcuItr}\underset{#2}{\mid}}
\newcommand{\afree}[1]{\textsf{asyncDelayedFree}(#1)}
\newcommand{\allocate}{\textsf{asyncDelayedFree}}
\newcommand{\typelocal}{\tau_{local}}
\newcommand{\typeglobal}{\tau_{global}::=\textsf{rcuRoot}  }
\newcommand{\rcutypes}{ \typelocal ::=   \textsf{undef} \mid \textsf{rcuItr} \,\, \rho \,\, \N \mid \rcunfd \mid \textsf{unlinked}  \mid \textsf{freeable}}
\newcommand{\typelocalmap}{\textsf{TypeLocal} : \var \mapsto \typelocal}
\newcommand{\command}[1]{\db{#1} \mstate \commandeq}
\newcommand{\commandm}[2]{\db{#1} #2 \commandeq}

\newcommand{\mstate}{(s,h,l,R,F)}
\newcommand{\commandeq}{\;\;\Downarrow_{\mathrm{tid}}\;\;\overset{\Delta}{=}}
\newcommand{\op}[2]{\begin{array}{rll}\command{#1} & #2\end{array}}
\newcommand{\opm}[3]{\begin{array}{rll}\commandm{#1}{#2} & #3\end{array}}
%%%
\newcommand{\return}[1]{\mathsf{return~\mathit{#1}}}
\newcommand{\bind}[2]{\mathsf{Bind}(#1;#2)}
\newcommand{\result}[1]{\mathsf{y = \mathsf{result}}}
\newcommand{\mbody}[2]{\mathsf{mbody}(#1,#2)}
\newcommand{\pre}{\textsf{pre}}
\newcommand{\assume}[1]{\textsf{assume}(#1)}
\newcommand{\lift}[1]{\lceil #1\rceil}
\newcommand{\erase}[1]{\lfloor #1\rfloor}
%%%
\newcommand{\M}{\ensuremath{\mathcal{M}}}
\newcommand{\s}{\ensuremath{\mathcal{S}}}
\newcommand{\R}{\ensuremath{\mathcal{R}}}
\newcommand{\N}{\ensuremath{\mathcal{N}}}
\newcommand{\condexpr}{e \in \textsf{CondExpr}}
%function definitions for paths

\newcommand{\lengthofpath}{$\mid \rho\mid \;=\;
  \begin{cases}
      0+ \mid \rho'\mid &  \rho = \rho_{i}^{k}\rho' \text{   where  } 0\leq i  \text{   and   } \rho_{i}^{k} = \epsilon \\
         k + \mid \rho'\mid & \rho = \rho_{i}^{k}\rho' \text{   where  } 0\leq i
  \end{cases}$

}
%%\textsf{Stabilize}(\Gamma,e) = \forall x \in \N[\fname] \ldotp \exists_{f_1,f_2 \in \fname}\ldotp \N[f_1] =  \N[f_2] \implies \N[f_1,f^{*}] \land \N[f_2,f^{*}]

\newcommand{\stabilizenext}{\textsf{StabilizeNext}(a,b,f)}
\newcommand{\stabilizepath}{\textsf{StabilzePath}(b,f,k)}
\newcommand{\stabilizecontext}{\textsf{StabilizeContext}(\Gamma,e) =
    \left\{\begin{array}{cl}
    e \in \textsf{PointsToExpr} \land
    \exists_{a,b,f}\ldotp  (a,b,f) = \textsf{PointsToExprMap}(e) \land \\
  \pi_{\N}(a) = \textsf{StabilizeNextMap}(a,b,f) \land \\
  \pi_{\rho}(b) = \textsf{StabilizePath}(b,f, \mid \pi_{\rho}(b)\mid ) \land\\
  \forall_{c \in \Gamma \land \textsf{TypeDepth}(c) >= \textsf{TypeDepth}(b)} \ldotp \pi_{\rho}(c) = \textsf{StabilizePath}(c,f,\mid\pi_{\rho}(c)\mid)
  \end{array}\right\}
}
%%%frame branch
\newcommand{\alternatingvars}[1]{\textsf{AbstractVars}(#1)}
\newcommand{\alternatingvarsdecl}{\textsf{AbstractVars}(\Gamma) \;=\; \{ x,y \mid  (x,y)\in \textsf{AbstractPairs}(\Gamma)\}}
\newcommand{\alternatetype}{ \text{x:}\textsf{rcuItr}\;\; \rho \;\; \N[f^{*}\mapsto y] \qquad   \text{y:}\textsf{rcuItr}\;\; \rho.\rho^{k} \;\; \N \text{    where    } \rho^{k}[k]=f^{*}}
\newcommand{\alternatingpath}[1]{f^{*} \; = \; f_1 \mid f_2  \; \text{where} \;  (f=f_1) \;\oplus \; ( f=f_2 )  }
\newcommand{\alternatingpathsdecl}[1]{\textsf{AbstractPairs}(#1) = \; \{ (\text{x},\text{y}) \mid \text{x} \in #1 \land \text{y} \in #1\}}
\newcommand{\alternatingpaths}[1]{\textsf{AbstractPairs}(#1)}
\newcommand{\pointstotuple}{\textsf{PointsToTuple}}
\newcommand{\pointstoexpr}{\textsf{PointsToExpr} = \{ x.f==y  \in \textsf{Expr} \mid \text{x} \in \var \; \land \; \text{y} \in \var \; \land \; f \in \fname \}}
\newcommand{\pointstopair}[1]{\textsf{PointsTo}(\text{#1})}
\newcommand{\projectfieldecl}{\pi_{f} : (\text{x},\text{y},f) \mapsto f}
\newcommand{\projectfield}[1]{\pi_{f}(#1)}
\newcommand{\projectpairdecl}{\pi_{\var} :(\text{x},\text{y},f) \mapsto (\text{x},\text{y})}
\newcommand{\projectpair}[1]{\pi_{\var}(#1)} % tuple,f.
\newcommand{\pointstoexprdecl}{\textsf{PointsToExpr} = \{e \mid  e = (a.f == b) \land \condexpr \land a,b \in \var \land f \in \fname  \} }
\newcommand{\pointstostmtdecl}{\textsf{PointsToExprMap}: \textsf{PointsToExpr}\mapsto \var \times \var \times \fname}
\newcommand{\pointstostmt}[1]{\textsf{PointsToExprMap}(#1) }
\newcommand{\freductionindexdecl}{\textsf{FieldReductionIndex}(\text{C},\text{x},\text{y},\text{irdc})\; = \;
  \left\{\begin{array}{cl}
    \forall_{0 \leq  i \leq n} \ldotp &  (x',y',f')_{i}  \in \pointstostmt{C} \land \\
    &(\text{x},\text{y})= \projectpair{(\text{x}',\text{y}',f')_{i}} \implies\\
    & \left\{\begin{array}{cl}
    \exists_{ firdc} \ldotp  & firdc \leq i \land  (firdc = 0 \implies firdc=irdc) \land \\
     & \left\{\begin{array}{cl}
    \forall_{0 \leq none \le firdc} \ldotp & (\text{x},\text{y}) \neq \projectpair{(\text{x}',\text{y}',f')_{none})}  \land \\
    &  firdc = \text{irdc}
      \end{array}\right\}
    \end{array}\right\}
  \end{array}\right\}
}

\newcommand{\projectsharpdecl}{\Pi_{\varphi_{e}}(f^{*}) \mapsto f_1 \qquad \Pi_{\lnot\varphi_{e}}(f^{*}) \mapsto f_2 }
\newcommand{\projectsharpdeclback}{\Pi(\varphi_{e})\mapsto e}

\newcommand{\frenamefielddecl}{\textsf{FieldRename}(\text{x},f,f')  \; = \; \text{x:T}[f/f'] }




\newcommand{\freductiondecl}{\textsf{FieldReduction}(\Gamma,\varphi_{e}) \; = \;
\left\{\begin{array}{cl}
  \textsf{FieldRename}(\text{x},f^{*},\Pi_{\varphi_{e}}(f^{*})),
  \textsf{FieldRename}(\text{y},f^{*},\Pi_{\varphi_{e}}(f^{*})) \mid (\text{x},\text{y}) =\projectpair{\pointstostmt{\Pi(\varphi_{e})}}
\end{array}\right\}
}

\newcommand{\freductiondeclneq}{\textsf{FieldReduction}(\Gamma,\lnot\varphi_{e}) \; = \;
\left\{\begin{array}{cl}
  \textsf{FieldRename}(\text{x},f^{*},\Pi_{\lnot\varphi_{e}}(f^{*})),
  \textsf{FieldRename}(\text{y},f^{*},\Pi_{\lnot\varphi_{e}}(f^{*})) \mid (\text{x},\text{y}) = \projectpair{\pointstostmt{\Pi(\varphi_{e})}}
\end{array}\right\}
  }


\newcommand{\gammafieldreduction}[4]{\textsf{ReduceAbstractPaths}(#3,#4,#2,#1) = \forall_{x,y \in \textsf{AbstractVars}(#4)} \ldotp \textsf{FieldReduction}(\text{x},\text{y},\varphi_{#3}) = #2 \land \textsf{FieldReduction}(\text{x},\text{y},\varphi_{#3}) = #1  } % x,y,gamma,c
\newcommand{\alterinxys}{\text{a:}\textsf{rcuItr}\;\; \rho \;\; \N[f_1\mapsto b] \qquad
  \text{b:}\textsf{rcuItr}\;\; \rho.f_1 \;\; \N  \text{  or }\text{b:}\textsf{rcuItr}\underset{r}{\mid} \rho.\{...f_1\} \;\; \N \\
  \text{a:}\textsf{rcuItr}\;\; \rho \;\; \N[f_2\mapsto b] \qquad
  \text{b:}\textsf{rcuItr}\;\; \rho.f_2 \;\; \N \text{  or  } \text{b:}\textsf{rcuItr}\;\; \rho.\{...f_2\} \;\; \N }
\newcommand{\reducedpathsdecl}[2]{\textsf{ReducedPaths}(#1,#2)\; = \; \left\{\begin{array}{cll}
  (a,b) \mid  &\text{a:}\textsf{rcuItr}\;\; \rho \;\; \N[f_1\mapsto b] \in #1 & \\
  &  \land  \text{b:}\textsf{rcuItr}\;\; \rho.\rho^{k} \;\; \N \in #1  & \text{     where    } \rho^{k}[k]=f_1 \\
  &  \land  \text{a:}\textsf{rcuItr}\;\; \rho \;\; \N[f_2\mapsto b] \in #2 & \\
  &  \land \text{b:}\textsf{rcuItr}\;\; \rho.\rho^{k} \;\; \N  \in #2 & \text{     where    } \rho^{k}[k]=f_2
  \end{array}\right\} }
\newcommand{\exprTopredDecl}{\textsf{ExprToPredicate}(e) \mapsto \varphi_{e}}
\newcommand{\reducedvars}{\textsf{ReducedVars}(\Gamma_1,\Gamma_2)}
\newcommand{\reducedvarsdecl}{\textsf{ReducedVars}(\Gamma_1,\Gamma_2)\;=\; \{\forall_{a,b \in \Gamma_1 \cup \Gamma_2} \ldotp (a,b) \in \textsf{ReducedPaths}(\Gamma_1,\Gamma_2) \}}
\newcommand{\reducedpaths}[2]{\textsf{ReducedPaths}(#1,#2)}
\newcommand{\abstractpaths}[4]{\textsf{AbstractPaths}(#1,#2,#3,#4) \; = \; \left\{\begin{array}{cl} a ,b \mid

  \begin{array}{cl}
 \forall_{a,b \in #1}\ldotp & \textsf{FieldRename}(a,#2,\rewritealternate{#2}{#3}{#4}) \land \\
  &\textsf{FieldRename}(a,#3,\rewritealternate{#2}{#3}{#4}) \land\\
  &\textsf{FieldRename}(b,#2,\rewritealternate{#2}{#3}{#4}) \land \\
  &\textsf{FieldRename}(b,#3,\rewritealternate{#2}{#3}{#4})
  \end{array}
   \end{array}\right\}
 }
\newcommand{\rewritealternatedecl}[3]{\textsf{AbstractField}(#1,#2,#3) = f^{*} \; \texttt{    where   } f^{*} \;=\;  \frac{\varphi_{#3}}{#1 \mid #2} }
  \newcommand{\rewritealternate}[3]{\textsf{AbstractField}(#1,#2,#3)}
\newcommand{\composerename}{\circ \text{  provides composing renaming of types ... removes duplicates? [TODO : May change this, keep this simple, now it is nothing, same as ;]. }}

%%%%%%%
\newcommand{\p}{\rho}

\newcommand{\ptriple}[3]{\{#1\}#2\{#3\}}
\newcommand{\atriple}[3]{\begin{array}{l}\{#1\}\\#2\\\{#3\}\end{array}}
\newcommand{\assert}[1]{\{#1\}}
\newcommand{\modifies}[1]{\textsf{Modifies}(#1)}

%\usepackage[usenames,dvipsnames]{color}
\definecolor{framed}{gray}{0.50}
\newcommand{\framed}[1]{\textcolor{framed}{\assert{#1}}}

% Make sure these are all done
\usepackage{todonotes}

\newcommand{\iso}[1]{\textrm{\color{magenta}\ensuremath{\llbracket}\textrm{Iso says:} #1\ensuremath{\rrbracket}}}
\newcommand{\colin}[1]{\textrm{\color{cyan}\ensuremath{\llbracket}\textrm{Colin says:} #1\ensuremath{\rrbracket}}}
%\newcommand{\tocite}[1]{\textrm{\color{green}$\llbracket$CITE: #1$\rrbracket$}}

\newcommand{\mypar}[1]{\textbf{#1}~\xspace}

\newcommand{\hallocprovided}{
  \texttt{ provided } s[(y,tid)\mapsto o], \;  h[o \mapsto \texttt{new}] \text{ and } \texttt{ default }(\textsf{FType}(f))\\
  \lambda o', f  \ldotp  \text{ if }  o = o' \;  \text{ skip } \text{ else }  h(o',f)
}
\newcommand{\gcprovided}{
  \texttt{ provided }  s[(x,tid)\mapsto o], \; (\forall f, o' \ldotp o\neq o' \Rightarrow h(o',f) = h'(o',f))) \\
  \text{ and } \forall f \ldotp h'(o,f)\; \textsf{ undefined }
}
\newcommand{\rcuwbeginprovided}{
 \textrm{provided }tid \notin R
}
\newcommand{\rcurbeginprovided}{
 \textrm{provided }tid \neq l
}
\newcommand{\rcurendprovided}{
 \textrm{provided } \; F \setminus tid \; \textrm{ as } \; \{ (T \setminus tid \},o) \mid (T,o) \in F \}
}
\newcommand{\gc}{
  \inferrule*[left={\scriptsize(Garbage-Collect)}]{}
           {
             \op{\texttt{gcStep}}{(s,h',l,R,F)}
           }
}
%%%
\newcommand{\rcuwbegin}{
  \inferrule*[left={\scriptsize(RCU-WriteBegin)}]{}
           {
            \opm{\texttt{WriteBegin}}{(s,h,\mathrm{unlocked},R,F)}{(s,h,\mathrm{tid},R,F)}
           }
}
\newcommand{\rcuwend}{
  \inferrule*[left={\scriptsize(RCU-WriteEnd)}]{}
           {
            \opm{\texttt{WriteEnd}}{(s,h,\mathrm{tid},R,F)}{(s,h,\textsf{unlocked},R,F)}
           }
}
\newcommand{\rcurbegin}{
  \inferrule*[left={\scriptsize(RCU-ReadBegin)}]{}{
    \opm{\texttt{ReadBegin}}{(s,h,l,R,F)}{(s,h,l,R \uplus \{ \mathrm{tid}\},F)}
  }
}
\newcommand{\rcurend}{
  \inferrule*[left={\scriptsize(RCU-ReadEnd)}]{}{
    \opm{\texttt{ReadEnd}}{( s,h,l,R \uplus \{ \mathrm{tid} \},F)}{(s,h,l,R,F \setminus \mathrm{tid}) }
  }
}

%%%
\newcommand{\asyncfree}{
\inferrule*[left={\scriptsize(Async-Free)}]{}
           {
            \op{\texttt{asyncDelayedFree}(x)}{(s,h,l,R,F \uplus \{ (R,S(x))\} )}
           }

}
\newcommand{\hallocate}{
  \inferrule*[left={\scriptsize(Heap-Allocate)}]{}
             {
                \op{\texttt{y=new}}{(s,h,l,R,F)}
             }
}
\newcommand{\supdate}{
  \inferrule*[left={\scriptsize(Stack-Update)}]{}
           {
             \op{\texttt{y=x}}{(s[(y,tid) \mapsto (x,tid)],h,l,R,F)}
           }
}
\newcommand{\hread}{
  \inferrule*[left={\scriptsize(Heap-Read)}]{}
             {
               \op{\texttt{y=x.f}}{((s[(y,tid) \mapsto h(s(x,tid),f)],h,l,R,F)}
             }
}
\newcommand{\hupdate}{
  \inferrule*[left={\scriptsize(Heap-Update)}]{}
             {
               \op{\texttt{x.f=y}}{(s,h[s(x,tid),f \mapsto s(y,tid)],l,R,F)}
             }
}
\newcommand{\tuigread}[1][T-UniqueGlobalRead]{
\infer[\textsc{#1}]{
  \Gamma \, , y:\_ \vdash_{M,R} y = G \dashv y: \textsf{rcuItr}\;\; \rho \;\; \N_{\emptyset} , \Gamma
}{
\begin{array}{c}
   G : \uiglobal \\
\end{array}
}
}

\newcommand{\tuisread}[1][T-UniqueStackRead]{
\infer[\textsc{#1}]{
  \begin{array}{c}
  \Gamma,\rcuitrn{y}{G}{0}{k+1}{\_} , x:\textsf{\_}  \vdash_{M,R} x = y \dashv  \rcuitrn{x}{y}{1}{2}{\_} , \rcuitrn{y}{G}{0}{1}{\_}, \Gamma \\
  \end{array}
}{
\begin{array}{c}
   G : \uiglobal \\
\end{array}
}
}

\newcommand{\toverwc}[1][T-OverWrite]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, x:\textsf{rcuItr}\;\; \rho \;\; \N([f \rightharpoonup \_])  \vdash_{M,R} x = x.f \dashv   x:\textsf{rcuItr}\;\; \rho' \;\; \N(\setminus_{f\rightharpoonup \_})  ,\,  \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}}  \qquad \mid \rho \mid +1 = \mid \rho' \mid  \\
\end{array}
}
}
%\qquad \aliaswith{z}{x} \rcuitr{z}{y}{k}{k+1}{x}\,

\newcommand{\treadstack}[1][T-ReadStack]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:{\_}, \rcuitrT{x}{y}{k}{k+1}{\_}   \vdash_{M,R} z=x \dashv  \rcuitrT{x}{y}{k}{k+1}{\_}, \rcuitrT{z}{y}{k}{k+1}{\_} , \Gamma \\
\end{array}
}{
\begin{array}{c}
 \\
\end{array}
}
}

\newcommand{\treadnext}[1][T-ReadNext]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:{\_},  x:\textsf{rcuItr}\;\; \rho \;\; \N([f\rightharpoonup \_])   \vdash_{M,R} z=x.f \dashv  x:\textsf{rcuItr}\;\; \rho \;\; \N(f\rightharpoonup \_ \setminus z),\;\;z:\textsf{rcuItr}\;\; \rho' \;\; \N \;, \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \qquad \mid \rho \mid + 1 = \mid \rho' \mid \\
\end{array}
}
}
%%%This is read and assignment for loop

\newcommand{\tloverwc}[1][T-OverLoopWrite]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma,  x:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N   \vdash_{M,R} x = x.f \dashv   \textsf{rcuItr}\;\; \rho\{...f\}_{\Delta} \,\, \N ,\,  \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}}  \\
\end{array}
}
}
\newcommand{\tlreadstackext}[1][T-ReadLoopStack1]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:{\_}, x:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N \\  \vdash_{M,R} z=x \dashv \\ x:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N, z:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N \, , \Gamma \\
\end{array}
}{
\begin{array}{c}
 \\
\end{array}
}
}
\newcommand{\tlreadstackinit}[1][T-ReadLoopStackInit]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N[f \mapsto x], x:\textsf{rcuItr}\;\; \rho f \{...f_{loop}\}_{\Delta} \,\, \N \\  \vdash_{M,R} z=x \dashv \\ x:\textsf{rcuItr}\;\; \rho f \{...f_{loop}\}_{\Delta} \,\, \N, z:\textsf{rcuItr}\;\; \rho\{...f\}_{\Delta} \,\, \N \, , \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \qquad f_{loop} = \epsilon \\
\end{array}
}
}
\newcommand{\tlreadnextone}[1][T-ReadLoopNextAbstract]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:\textsf{rcuItr}\;\; \rho f_1\{...f_{loop}\}_{\Delta} \,\, \N, x:\textsf{rcuItr}\;\; \rho\{...f_1\}_{\Delta} \,\, \N[f_1\mapsto z] \\  \vdash_{M,R} z=x.f_2 \dashv \\  x:\textsf{rcuItr}\;\; \rho\{...f_1\}_{\Delta} \,\, \N[\frac{}{f_1 \mid f_2}\mapsto z],\, z:\textsf{rcuItr}\;\; \rho\{...f_2\}_{\Delta} \,\, \N \,, \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \qquad f_{loop}=\epsilon  \\
\end{array}
}
}
\newcommand{\tlreadnexttwo}[1][T-ReadLoopNextExtend]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:\textsf{rcuItr}\;\; \rho f_1\{...f_{loop}\}_{\Delta} \,\, \N, x:\textsf{rcuItr}\;\; \rho\{...f_1\}_{\Delta} \,\, \N[f_1\mapsto z] \\  \vdash_{M,R} z=x.f_1 \dashv \\  x:\textsf{rcuItr}\;\; \rho\{...f_1\}_{\Delta} \,\, \N[f_1\mapsto z],\, z:\textsf{rcuItr}\;\; \rho\{...f_1\}_{\Delta} \,\, \N \,, \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \qquad f_{loop}=\epsilon  \\
\end{array}
}
}
\newcommand{\tlreadnextthree}[1][T-ReadLoopNext3]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, z:{\_}, x:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N \\  \vdash_{M,R} z=x.f \dashv \\  x:\textsf{rcuItr}\;\; \rho\{...f_{loop}\}_{\Delta} \,\, \N[f\mapsto z],\, z:\textsf{rcuItr}\;\; \rho\{...f\}_{\Delta} \,\, \N \,, \Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \\
\end{array}
}
}
\newcommand{\approximatedecl}{\textsf{Approximate}(x:\textsf{rcuItr}\;\; \rho \;\; \N, \Delta)\mapsto x:\textsf{rcuItr} \;\;\rho\{...\} \;\;\N}
\newcommand{\approxtype}[2]{\textsf{Approximate}(#1,#2)}
\newcommand{\gammaapproxdecl}{\Gamma_1 \approx_{\Delta} \Gamma_2 \text{   where  } \Gamma_2 = \{\approxtype{x:\text{T}}{\Delta} \mid \forall_{x:\text{T}\in\Gamma_1}  \}}
\newcommand{\gammapprox}[3]{#1 \approx_{#3} #2}
% \qquad \aliaswith{t}{x} \rcuitr{t}{y}{k}{k+1}{z}
\newcommand{\tunlinknode}[1][T-UnlinkANode]{
\infer[\textsc{#1}]{
\begin{array}{c}
\Gamma, \,
x:\textsf{rcuItr}\;\; \rho \;\; \N([f_1\rightharpoonup z]) , \;\;\;
z:\textsf{rcuItr} \;\; \rho' \;\; \N'([f_2\rightharpoonup r]) , \;\;\; r:\textsf{rcuItr}\;\; \rho'' \;\; \N'' \\
\vdash_{M} x.f=r \dashv \\
z:\unlinked, \,
x:\textsf{rcuItr}\;\; \rho \;\; \N(f_1\rightharpoonup z \setminus r), \;\;\;
r:\textsf{rcuItr}\;\; \rho' \;\; \N'' \;\;
\Gamma \\
\end{array}
}{
\begin{array}{c}
  \ftype{ }{\textsf{rcu}} \qquad \mid\rho\mid + 1= \mid \rho' \mid  \qquad \mid \rho' \mid + 1 = \mid \rho'' \mid  \\ \forall_{f\in dom(\N')} \ldotp f\neq f_2 \implies \N'([f\rightharpoonup \textsf{Null}])  \\
\end{array}
}
}
%\qquad  \aliaswith{t}{z} t: \unlinked , \,
\newcommand{\tafree}[1][T-AsyncFree]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, x:\unlinked \vdash_{M} \afree{x} \dashv x:\udef , \Gamma \\
\end{array}
}{
\begin{array}{c}
   \\
\end{array}
}
}

\newcommand{\tallocate}[1][T-Allocate]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, x:\_ \vdash_{M} x = \NEW \dashv x:\textsf{rcuFresh} \;\; \N_{\emptyset} , \, \Gamma \\
\end{array}
}{
\begin{array}{c}
   \\
\end{array}
}
}

\newcommand{\tsetfieldfresh}[1][T-SetFieldFresh]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, \rcuf{p} \;\;, x:\textsf{rcuItr}\;\; \rho \;\; \N([f\rightharpoonup z]) \;\; \\
  \vdash_{M} p.f = x.f \dashv \\
  \rcunf{p}{f}{z} ,  \;\;, x:\textsf{rcuItr}\;\; \rho \;\; \N([f\rightharpoonup z])\;\;, \Gamma \\
\end{array}
}{
\begin{array}{c}
   \\ f \notin \N
\end{array}
}
}


\newcommand{\tlinkfreshf}[1][T-LinkFreshNext]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, \:\;
z:\textsf{rcuItr}\;\; \rho \;\; \N([f\rightharpoonup x]) \;\;,
x:\textsf{rcuItr}\;\; \rho' \;\; \N \;\;,
r:\textsf{rcuFresh} \;\; \N([f\rightharpoonup x]) \\
  \vdash_{M} z.f = r \dashv \\
z:\textsf{rcuItr}\;\; \rho \;\; \N(f\rightharpoonup x\setminus r) \;\;,
r:\textsf{rcuItr}\;\; \rho' \;\; \N([f\rightharpoonup x]) \;\; ,
x:\textsf{rcuItr}\;\; \rho'' \;\; \N
 \;\; , \Gamma \\
\end{array}
}{
\begin{array}{c}
   \ftype{ }{\textsf{rcu}} \qquad \mid\rho\mid + 1= \mid \rho' \mid  \qquad \mid \rho' \mid + 1 = \mid \rho'' \mid \\
\end{array}
}
}

\newcommand{\tlinkfresh}[1][T-LinkFresh]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma, \;\;
  p:\textsf{rcuItr}\;\; \rho \;\; \N([f\rightharpoonup o]) \;\;,
  o:\textsf{rcuItr}\;\; \rho' \;\; \N' \;\;, n:\textsf{rcuFresh} \;\; \N'' \\
  \vdash_{M} p.f = n \dashv \\
  p:\textsf{rcuItr}\;\; \rho \;\; \N(f \rightharpoonup o \setminus n) \;\;,
  n:\textsf{rcuItr}\;\; \rho' \;\; \N'' \;\;,
  o:\unlinked, \;\;  \Gamma
\end{array}
}{
\begin{array}{c}
   \ftype{ }{\textsf{rcu}}  \qquad \mid \rho \mid + 1= \mid \rho' \mid \qquad \N' = \N'' \\
\end{array}
}
}
\newcommand{\tloop}[1][T-Loop]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \Gamma \vdash \text{C}^{*} \dashv  \Gamma \\
\end{array}
}{
  \begin{array}{c}
     \Gamma \vdash C \dashv \Gamma
\end{array}
}
}
\newcommand{\tloopold}[1][T-LoopOld]{
\infer[\textsc{#1}]{
  \begin{array}{c}
 \Gamma_1',\Gamma_1'' \vdash \text{C}^{\Delta} \dashv   \Gamma_1'''',\Gamma_1''    \\
%  \tloopweaken\\
\end{array}
}{
  \begin{array}{c}
    \tframeloop \\
       \Delta = 0 \implies \Gamma_1' =\Gamma_1''''  \land \\
       \Delta >0 \implies
       \left\{\begin{array}{cl}
       \Gamma_1'\approx_{\Delta}\Gamma_1''' \land \left\{\begin{array}{cl}
       \exists_{0<\varepsilon \leq\Delta} \ldotp 0<\varepsilon<\Delta  \implies  \Gamma_1'''\vdash C \dashv \textsf{ContexRename}(\Delta,\Gamma_1''',\Gamma_1'''') \land \\
       \varepsilon = \Delta  \implies \Gamma_1'''\vdash C \dashv \Gamma_1''''
       \end{array}\right\}
       \end{array}\right\} \\
\end{array}
}
}

% \qquad \rcuitrT{y}{G}{0}{k+1}{\_} \\  \rcuonly{\rcufiltera{\Gamma_1}}{k}{y} \\ \rcuonly{\rcufiltera{\Gamma_2}}{k'}{y} \\   \Gamma_1 \vdash C \dashv   \Gamma_2

\newcommand{\tpar}[1][T-Par]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma_1 \; \Gamma_2  \vdash_{M,R} \bar{s_1} || \bar{s_2} \dashv  \Gamma'_1 \; , \Gamma'_2 \\
\end{array}
}{
\begin{array}{c}
   \Gamma_1 \vdash_{R} \bar{s_1} \dashv   \Gamma'_1  \qquad   \Gamma_2 \vdash_{M,R} \bar{s_2} \dashv   \Gamma'_2\\
\end{array}
}
}
\newcommand{\tllinkfresh}[1][T-LoopExtLinkFresh]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \{\text{TODO}\} \\
\end{array}
}{
\begin{array}{c}
 \{\text{TODO}\}\\
\end{array}
}
}
\newcommand{\tllinkfreshnext}[1][T-LoopExtLinkFreshNext]{
\infer[\textsc{#1}]{
\begin{array}{c}
\{\text{TODO}\}\\
\end{array}
}{
\begin{array}{c}
\{\text{TODO}\} \\
\end{array}
}
}
\newcommand{\tlunlink}[1][T-LoopExtUnlink]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \{\text{TODO}\} \\
\end{array}
}{
\begin{array}{c}
 \{\text{TODO}\}\\
\end{array}
}
}
\newcommand{\tskip}[1][T-Skip]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \Gamma \vdash \textsf{skip} \dashv   \Gamma \\
\end{array}
}{
\begin{array}{c}
 \\
\end{array}
}
}
\begin{comment}
\newcommand{\tframebranch}[1][T-FrameRCUBranch]{
\infer[\textsc{#1}]{
\begin{array}{c}
 \Gamma_{1\text{C}_1}, \, \Gamma'_{1\text{C}_1}  \vdash C_1 \dashv \Gamma'_{23R},\, \Gamma_{abs} \qquad  \Gamma_{1\text{C}_1}, \, \Gamma'_{1\text{C}_2}  \vdash C_2 \dashv  \Gamma'_{23R},\, \Gamma_{abs}
\end{array}
}{
  \begin{array}{c}
    \Gamma_1  \vdash \text{if e C_1 else C_2}  \dashv  \Gamma_4 \\
    \exists_{\Gamma'_1} \ldotp \Gamma'_1= \textsf{AbstractVars}(\Gamma_1) \qquad \exists_{\Gamma''_1}\ldotp \Gamma_1 = \Gamma'_1 \uplus \Gamma''_1\\
    \Gamma''_1,\Gamma'_1\vdash C_1 \dashv   \Gamma_2 \qquad \Gamma''_1,\Gamma'_1 \vdash C_2 \dashv \Gamma_3

\end{array}
}
}
\end{comment}

\newcommand{\tbranch}[1][T-Branch]{
\infer[\textsc{#1}]{
  \begin{array}{c}
 \Gamma,x:\textsf{rcuItr}\;\; \rho \;\; \N([f_1\mid f_2 \rightharpoonup z]) \vdash \text{if e C$_1$ else C$_2$} \dashv \Gamma_4
\end{array}
}{
  \begin{array}{c}
    e \in \textsf{PointsToExpr} \qquad e \text{ is } x.f_1==z \\
 \Gamma,x:\textsf{rcuItr}\;\; \rho \;\; \N([f_1 \rightharpoonup  z]) \vdash C_1 \dashv\Gamma_4 \qquad \Gamma,x:\textsf{rcuItr}\;\; \rho \;\; \N([f_2 \rightharpoonup z]) \vdash C_4 \dashv \Gamma_4
\end{array}
}
}

\newcommand{\tframe}[1][T-Frame]{
\infer[\textsc{#1}]{
  \begin{array}{c}
 \Gamma, \; \Gamma_1 \vdash C \dashv \Gamma, \; \Gamma_2
\end{array}
}{
  \begin{array}{c}
 \Gamma_1 \vdash C\dashv \Gamma_2  \qquad \textsf{RcuModifiedOnly}(C)
\end{array}
}
}

%%\seprcubd{\Gamma_1}{\Gamma_2}{\Gamma_3}{\Gamma_4}{C_1}{C_2}{\Gamma}

%\Gamma''_1,\Gamma'_3\vdash \text{C$_1$} \dashv   \Gamma_2 \qquad \Gamma''_1,\Gamma'_1 \vdash \text{C$_2$} \dashv \Gamma_3 \qquad \Gamma_{23} = \Gamma_2 \cup \Gamma_3  \\
   %  \Gamma_{23R} = \textsf{ReducedVars}(\Gamma_2,\Gamma_3) \qquad \Gamma'_{23R}=\Gamma_{23} - \Gamma_{23R} \qquad \Gamma_{abs} = \textsf{AbstractPaths}(\Gamma_{23R},\text{f$_1$},\text{f$_2$},e) \qquad \Gamma_4 =  \Gamma'_{23R},\, \Gamma_{abs}


\newcommand{\tchoosef}[1][T-Choose-First]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma_1  \vdash C_1 \dashv  \Gamma_2 \\
\end{array}
}{
\begin{array}{c}
   \Gamma_1 \vdash C_1 \dashv   \Gamma_2  \qquad   \Gamma_1 \vdash C_2 \dashv   \Gamma_3\\
\end{array}
}
}
\newcommand{\tchooses}[1][T-Choose-Second]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma_1  \vdash C_2 \dashv  \Gamma_3 \\
\end{array}
}{
\begin{array}{c}
   \Gamma_1 \vdash C_1 \dashv   \Gamma_2  \qquad   \Gamma_1 \vdash C_2 \dashv   \Gamma_3\\
\end{array}
}
}
\newcommand{\tseq}[1][T-Seq]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \Gamma_1  \vdash \bar{s_1} \; ; \; \bar{s_2} \dashv  \Gamma_3 \\
\end{array}
}{
\begin{array}{c}
   \Gamma_1 \vdash \bar{s_1} \dashv   \Gamma_2  \qquad   \Gamma_2 \vdash \bar{s_2} \dashv   \Gamma_3\\
\end{array}
}
}

\newcommand{\tshuffle}[1][T-Exchange]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \Gamma, \, x:T , \, y:T' , \, \Gamma' \vdash \bar{s} \dashv   \Gamma'' \\
\end{array}
}{
\begin{array}{c}
   \Gamma, \, y:T' , \, x:T, \, \Gamma' \vdash \bar{s} \dashv   \Gamma'' \\
\end{array}
}
}
\newcommand{\tconsequence}[1][T-Conseq]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \Gamma \vdash C \dashv \Gamma''' \\
\end{array}
}{
\begin{array}{c}
   \Gamma \subt \Gamma'  \qquad \Gamma' \vdash C \dashv \Gamma'' \qquad \Gamma'' \subt \Gamma''' \\
\end{array}
}
}

%%Sub typing
\newcommand{\nsubone}[1][N-Sub-F1]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash \, \N \subt \N'  \\
\end{array}
}{
\begin{array}{c}
   \text{T } = \textsf{rcuItr}\;\; \rho \;\; \N([f_1 \rightharpoonup y]) \\
   \text{T}'=\textsf{rcuItr}\;\; \rho \;\; \N'([f^{*} \rightharpoonup y])
\end{array}
}
}

\newcommand{\nsubtwo}[1][N-Sub-F2]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash \, \N \subt \N'  \\
\end{array}
}{
\begin{array}{c}
   \text{T} = \textsf{rcuItr}\;\; \rho \;\; \N([f_2 \rightharpoonup y]) \\
   \text{T}'=\textsf{rcuItr}\;\; \rho \;\; \N'([f^{*} \rightharpoonup y])
\end{array}
}
}
\newcommand{\nsubthree}[1][N-Sub]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash \, \N_1 \subt \N \, \land \, \N_2 \subt \N  \\
\end{array}
}{
\begin{array}{c}
  \N_1[f_2\mapsto x] \qquad  \N_2[f_1\mapsto x] \qquad  \N[f^{*} \mapsto x] \qquad f^{*} = \frac{\varphi_{e}}{f_1 \mid f_2}\\
\end{array}
}
}



\newcommand{\psubone}[1][P-Sub]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash  \rho' \subt \rho'' \\
\end{array}
}{
  \begin{array}{c}
  \rho'' = \rho'\rho^{k}
\end{array}
}
}

\newcommand{\psubtwo}[1][P-Sub-Ref]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash  \rho \subt \rho  \\
\end{array}
}{
  \begin{array}{c}
  \\
\end{array}
}
}

\newcommand{\psubthree}[1][P-Sub-Trans]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash  \rho' \subt \rho'''  \\
\end{array}
}{
  \begin{array}{c}
    \vdash  \rho' \subt \rho'' \qquad
    \vdash  \rho'' \subt \rho'''
\end{array}
}
}

\newcommand{\csubfour}[1][C-Sub-Emp]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash  \epsilon \subt \epsilon  \\
\end{array}
}{
  \begin{array}{c}
\end{array}
}
}



\newcommand{\tsubase}[1][S-Base]{
\infer[\textsc{#1}]{
\begin{array}{c}
 \vdash \, \text{T} \prec_{\varepsilon = 0} \text{T'} \qquad \text{T} =_{\varepsilon = 0} \text{T'} \\
\end{array}
}{
\begin{array}{c}
 \varepsilon = 0 \\
\end{array}
}
}
\newcommand{\tsubreflexive}[1][S-Reflexive]{
\infer[\textsc{#1}]{
\begin{array}{c}
 \vdash \, \text{T} \preceq_{\varepsilon} \text{T} \\
\end{array}
}{
\begin{array}{c}
  \\
\end{array}
}
}
\newcommand{\tsubtransitive}[1][S-Transitive]{
\infer[\textsc{#1}]{
\begin{array}{c}
  \vdash \, \text{T}_1 \preceq_{2\varepsilon} \text{T}_3 \\
\end{array}
}{
\begin{array}{c}
    \vdash \, \text{T}_1 \preceq_{\varepsilon} \text{T}_2 \qquad \vdash \, \text{T}_2 \preceq_{\varepsilon} \text{T}_3 \\
\end{array}
}
}
\newcommand{\tsubempty}[1][S-Empty]{
\infer[\textsc{#1}]{
\begin{array}{c}
   \vdash \, \epsilon \preceq_{\varepsilon} \epsilon \\
\end{array}
}{
\begin{array}{c}
    \\
\end{array}
}
}
\newcommand{\subcontextone}[1][C-Sub-Red-notsure]{
\infer[\textsc{#1}]{
  \begin{array}{c}
        \vdash \Gamma_1  \subt \Gamma \, \land \, \Gamma_2 \subt \Gamma \\
\end{array}
}{
  \begin{array}{c}
    \textsf{ReduceAbstractPaths}(e,\Gamma,\Gamma_1,\Gamma_2) \\
\end{array}
}
}
\newcommand{\subeqcontexttwo}[1][C-Sub-Abs-notsure]{
\infer[\textsc{#1}]{
  \begin{array}{c}
        \vdash \Gamma'  \subt \Gamma \\
\end{array}
}{
  \begin{array}{c}
    \textsf{ReducedVars}(\Gamma_1,\Gamma_2) = \Gamma' \qquad \textsf{AbstractPaths}(\Gamma',f_1,f_2,e) =\Gamma
\end{array}
}
}
\newcommand{\subeqcontextthree}[1][C-Sub]{
\infer[\textsc{#1}]{
  \begin{array}{c}
        \vdash \Gamma,\, x:\text{T}  \subt \Gamma',\, x:\text{T'} \\
\end{array}
}{
  \begin{array}{c}
    \vdash\Gamma \subt \Gamma' \qquad
    \vdash \text{T}  \subt \text{T'}
\end{array}
}
}



\begin{comment}
\newcommand{\tsubcons}[1][S-Cons]{
\infer[\textsc{#1}]{
  \begin{array}{c}
        \vdash \Gamma, \, \text{x:T}  \prec \Gamma',\, \text{x':T'}  \\
\end{array}
}{
\begin{array}{c}
      \vdash \Gamma \prec \Gamma'  \qquad \text{T} \prec \text{T'}  \\
\end{array}
}
}

\newcommand{\tsubdrop}[1][S-Drop]{
\infer[\textsc{#1}]{
  \begin{array}{c}
      \vdash \Gamma \prec \Gamma' \\
\end{array}
}{
\begin{array}{c}
    \vdash \Gamma, \, \text{x:T}  \prec \Gamma'  \\
\end{array}
}
}
\end{comment}
%%%%
\newcommand{\tsmovewrite}{
\infer[\textsc{Move-To-RCUWrite}]{
\begin{array}{c}
  \Gamma \vdash \textsf{RCUWrite}\, x.f \textsf{ as }y\textsf{ in }\{C\} \\
\end{array}
}{
 \begin{array}{c}
 \Gamma, y : \textsf{rcuItr} \vdash_M C \dashv \Gamma' \qquad
 \ftype{}{\textsf{RCU}} \\
 \textsf{NoFresh}(\Gamma') \qquad
\textsf{NoUnlinked}(\Gamma')
\end{array}
}
}
\newcommand{\tsmoveread}{
  \infer[\textsc{Move-To-RCURead}]{
\begin{array}{c}
\Gamma \vdash \textsf{RCURead}\, x.f \textsf{ as }y\textsf{ in }\{C\} \\
\end{array}
}{
\begin{array}{c}
\ftype{}{\textsf{rcu}} \\
\Gamma, y : \textsf{rcuItr} \vdash_R C \dashv \Gamma' \\
\end{array}
}
}

\newcommand{\meta}{
\[
\begin{array}{rclcl}
a & \textsf{atoms} \\
s & \textsf{statement}  & & s::= & a \mid skip \mid s;s \mid s+s \mid s \parallel s \mid s^{*} \\
n,x,y,z,o,p & \textsf{variables}  & &  \Gamma::= & . \mid \Gamma,x:t \\
t & \textsf{types}  & & t::= & \mathsf{int} \mid \mathsf{bool} \mid \mathsf{struct} \; sn \{ \overline{fld}\} \mid T\\
T & \textsf{Qualified types} & & T::= & t<rt> \\
f & \textsf{field name}  & & fld::= & t \; f \mid  t<rt> \; f\\
rb & \textsf{rcu blocks}  & & rb::= & \mathsf{ReadBegin} \, ; \bar{s}  \, ; \mathsf{ReadEnd} \mid \mathsf{WriteBegin} \, ; \bar{s}  \, ;  \mathsf{WriteEnd} \\
fld & \textsf{field decl}  & & rt::= & \mathsf{rcu} \mid \mathsf{rcuItr} \mid \mathsf{rcuRoot} \\
sn & \textsf{struct name}   & &  i,j & \mathsf{nat (indices)}  \\

\end{array}
\]\\
\[
\begin{array}{rclcl}
\alpha &::= & \mathsf{skip} \mid \mathsf{x.f = y} \mid \mathsf{y=x} \mid \mathsf{y =x.f}  \mid \mathsf{y =new}\mathsf{Free}\mathsf{(\textsf{x})} \mid \mathsf{Sync}
 \end{array}
\]
}

\newcommand{\grammar}{
\[
\begin{array}{rclcl}
\alpha &::= & \mathsf{skip} \mid \mathsf{x.f = y} \mid \mathsf{y=x} \mid \mathsf{y =x.f}  \mid \mathsf{y =new} \mid  \mathsf{Free}\mathsf{(\textsf{x})} \mid \mathsf{Sync}                   
\end{array}\quad
\textsf{Sync} \overset{\Delta}{=} \texttt{\textsf{SyncStart};\textsf{SyncStop}}
\]
}


\lstset{numbersep=1.7pt,xrightmargin=-0.7cm,xleftmargin=0.7em,numbers=left,columns=fullflexible,escapechar=`,mathescape,language=Java,basicstyle=\small\ttfamily,keywordstyle=\color{blue},morekeywords={ReadBegin,ReadEnd,WriteBegin,WriteEnd,SyncStart,SyncStop,Free,foreach,in,var,consume,readable,r,isolated,writable,immutable}}

%\lstset{
%  xleftmargin=1em,      % <-- change this to a suitable length
%  xrightmargin=-0.5cm,  % <-- change this to a suitable negative length
%  numbersep=2pt,        % default 10pt
%  numbers=left,
%  numberstyle=\footnotesize,
%  basicstyle=\ttfamily,
%  breaklines=true}
\usepackage{lipsum} % for dummy text
\usepackage{comment}

\renewcommand{\and}{\\}

\usepackage{microtype}%if unwanted, comment out or use option "draft"
%\bibliographystyle{plainurl}% the recommended bibstyle

%\bibliography{references/paper}
\begin{document}
%
\title{Modal Verification Patterns for Systems}
%
%\titlerunning{RCU Types}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Ismail Kuru \orcidID{0000-0002-5796-2150} \and
Colin S. Gordon \orcidID{0000-0002-9012-4490}}
%%%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{Kuru and Gordon}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Drexel University \\ %\and
%%%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
\email{\{ik335,csgordon\}@drexel.edu}}%\\
%%%\url{http://www.springer.com/gp/computer-science/lncs} \and
%%%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%%%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution

%\input{chapters/squeezing.tex}
\begin{abstract}
 Although they differ in the functionality they offer, low-level systems exhibit certain patterns of design and utilization of computing resources.
\begin{comment}
  In this paper, we argue the position that \emph{modalities}, in the sense of modal logic, 
  should be a go-to approach when specifying and verifying low-level systems code.
  We explain how the concept of a \emph{resource context} helps guide the design of new modalities
  for verification of systems code, and
  we justify our perspective by discussing prior systems that have used modalities for systems verification
  successfully, arguing that they fit into the verification design pattern we articulate,
  and explaining how this approach might apply to other systems verification challenges.
\end{comment}
  \todo[inline]{Colin says: how about this framing instead of the second paragraph above:}
  
  In this paper we examine how modalities have emerged as a common structure in formal verification
  of low-level software, and explain how many recent examples naturally share 
  common structure in the relationship between the modalities and software features they are used
  to reason about.
  We explain how the concept of a \emph{resource context} (a class of system resources
  to reason about) naturally corresponds to families of modal operators indexed by
  system data, and how this naturally leads to using modal assertions to describe
  \emph{resource elements} (data in the relevant context).
  We also describe extensions of this idea to ongoing work in formal verification
  of filesystems code.
  \todo[inline,color=cyan]{Ismail, why did you comment out my suggestion and keep the original?}
  \todo[inline,color=pink]{Sorry, I thought I did the other way}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>00000000.0000000.0000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>00000000.00000000.00000000</concept_id>
%  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%
%%%
%%% Keywords. The author(s) should pick words that accurately describe
%%% the work being presented. Separate the keywords with commas.
%\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for, Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
%\maketitle

\section{Introduction}
Although they differ in the functionality they offer, low-level systems exhibit certain patterns of design and utilization of computing resources.
Systems software, in general, interfaces with an underlying computing substrate such that any software system at any higher level in the software stack can (at least indirectly) utilize the resources of the machine.
The last layer of software before the hardware is naturally critical to the correctness of an overall system,
as essentially all software built on top of it assumes its correctness.
%Assuming verified hardware, this makes the correctness of the system software critical to the rest of the software stack. 
%Because they interface with the hardware, abstractions and their implementations are intricate and error-prone. 
And because hardware is complex and highly diverse, the implementation of those lowest layers
of the software stack is typically intricate and naturally error-prone, despite how critical its correctness is.
Typically systems software has, as a primary focus, the task of \emph{abstracting} from hardware details to simplify 
the construction of higher layers of the stack.

% \subsection{Conclusion \& Contributions}
% \paragraph{Conclusion} 
Low-level systems software exhibits certain patterns in their designs, especially when interacting with computing resources. 
Exploiting certain patterns while designing software has been an important field of study.
This survey examines these common patterns in systems software, and their relationships
to modalities. We believe that certain properties of modalities enable us to understand and 
address the verification challenges of these systems by tailoring specification and proof
to the same patterns used to design the systems.

\paragraph{Contributions} We argue that modal abstractions can be used to identify and abstract system verification challenges. We justify our perspective by discussing prior systems that have successfully used modalities for system verification, arguing that they fit into the verification design pattern we articulate, and explaining how this approach might apply to other systems' verification challenges.
\begin{itemize}
\item \textbf{Identifying System Verification Challenges}: We start with identifying common patterns in system verification: \emph{virtualization, sharing, and translation}.
\item \textbf{Introducing the Concept of \emph{Resource}}: Then we discuss the concept of \emph{resource} which has already been an essential concept in the design of systems. Inspired by the concept of resource in the systems, we define what a resource and its context are in the modal abstractions.
\item \textbf{Introducing the Concept of \emph{Nominals for System Resources}}:
Nominalization enables identifying a resource in a context. 
For example, a transaction is a context of resources of in-memory updated disk blocks. i
The transaction identifier is used to associate a transaction with a disk-block to be persisted so that, 
in case of a crash while persisting updated disk-blocks, the filesystem can rollback the 
\emph{already persisted} disk-blocks of the transaction, and reach to the previous consistent disk state. 
To be able to do so, both the updated in-memory disk blocks and the transaction must refer to the transaction 
identifier -- \emph{strong nominalization}. In another example, virtual memory references (resources) in 
an address space (resource context), which can be uniquely identified with a root address (the nominal) 
of its page-table tree, are \emph{agnostic} to the address space that they are in. 
However, they can only be accessed (be valid) in the address space to which they are agnostic. 
However, then an address space switch happens, the virtual memory references of the previous address 
space must be made inaccessible. To be able to do so, although virtual memory references do not hold any 
piece of information related to their address space, we still have to associate them. 
We call this kind of unilateral nominalization -- \emph{weak nominalization}.
\todo[inline,color=cyan]{I left this paragraph alone, mostly, but it's too detailed for the new intro section position here}
\item \textbf{Taxonomy of Current Modal Approaches in System Verification}: Based on these concepts defined, we summarize contemporary verification efforts using modal abstractions. We choose them from different domains, for example, reasoning about weak memory models and storage persistence ~\cite{tejthesis,perennialgit,tejperennial19,larsnextgen25,fsl,fsl++,derekrustbelt20,kuru2024modalabstractionsvirtualizingmemory,amalreal2024} because we would like to justify that our \emph{definitions} are not domain dependent.
\end{itemize}


\section{Background on Modal Logics for Programs, and Low-Level Systems Software}

This section briefly recalls related concepts from modal logic of particular relevance
to our taxonomy, which should be familiar but benefit from fixing notation (Section \ref{sec:modalbackground}),
and enough general background on the systems software concepts organized by our taxonomy
for the connections to modalities to be clear (Section \ref{sec:background}).

\subsection{Hybrid Logic Dynamic Logic and Nominals in Program Logics}
\label{sec:modalbackground}


\paragraph{Dynamic Logic's Satisfaction Operator for Program Logics}
We believe that for the kinds of informal reasoning and the sorts of data structures discussed in the previous section, ideas from modal logic are a promising approach to formalization and verification.
Broadly speaking modal logics incorporate \emph{modal operators}, which take as arguments a proposition expected to be true in another time~\cite{pnueli1977temporal}, place~\cite{gordon2019modal,goranko1996hierarchies,areces2001hybrid,gargov1993modal,murphy2004symmetric}, circumstance or point-of-view~\cite{hintikka1962knowledge,halpern1985guide}, and result in a proposition true in the \emph{current} time, place, or circumstance at which the truth of the use of the modal operator is being evaluated. Classic examples include modal necessity $\square P$ describing that $P$ is \emph{necessarily} true, $\mathsf{G}~P$ meaning $P$ is true \emph{globally} (i.e., forever from this time onwards in temporal logics), or $K_i(P)$ describing that a particular participant $i$ \emph{knows} that $P$ is true. The latter is an example of \emph{multimodal} logic, where there is an family of modalities (modal operators) parameterized by some dimension of interest (there, participants).
A closely related variant of multimodal logic is \emph{dynamic logic}~\cite{pratt1976semantical}, a logic of weakest preconditions~\cite{dijkstra-75} which works with modalities of the form $[p](P)$, which states that \emph{in the current program state}, \emph{if} program $p$ is run then afterwards $P$ will hold (modulo non-termination).
This same idea is used to encode Hoare triples in the Iris program logic~\cite{krebbers2017essence}, using the same encoding as in Pratt's original presentation, where a Hoare triple $\{P\}C\{Q\}$ is encoded as $P\rightarrow[C](Q)$.
A hallmark of a unary logical operator $M$ being a modality is if $M$ satisfies a property akin to $(\varphi\rightarrow\psi)\rightarrow M(\varphi)\rightarrow M(\psi)$, which roughly states that modus ponens holds under the modality.\footnote{Afficionados of modal logic will note that this property is not quite Axiom K (which requires the initial implication to also hold under $M$), but follows from K and a necessitation rule $\varphi\rightarrow M(\varphi)$. Non-necessitive modalities typically satisfy the weaker property we call out above.} This and related laws allow transporting general reasoning into the specific contexts or hypotheticals modeled by the modalities.

\paragraph{Hybrid Logic} Hybrid logics~\cite{blackburn1995hybrid,goranko1996hierarchies,gargov1993modal,areces2001hybrid,brauner2010hybrid} are a class of modal logics that add two primary new concepts to a logic. Nominals $\l \in \textsf{Loc}$ uniquely identify points in a model (e.g., a particular state), so there is exactly one point in the model where a nominal $\loc$ is true. Satisfaction operators are modal operators indexed by nominals, which enable claims about the truth of another proposition at some arbitrary point in the model identified by a nominal. Traditionally $@_l\varphi$ asserts that $\varphi$ is true in the state identified by $l$. Conceptually, given our context, we might like $l$ to indicate truth in a particular data structure $l$.  However, most prior work on hybrid logics works in classical logics rather than a substructural setting like separation logic. 
%We adapt these ideas to assertions of the form $@[l]\varphi$ asserts that $\varphi$ is true in a data structure with \emph{logical} state $\sigma$ \emph{at location} $l$.
\todo{seems out of place?} \todo[inline,color=pink]{addressed}
In practice we must design variants of this for each kind of structure we wish to apply this to, e.g. tree nodes, free list, or snapshots.
%This section provides details of and models for each modal abstraction, focusing on how each data structure can act as a model for a data-structure-specific logic.
%We continue to motivate the development with aspects of the write amplification process described in the previous section; Section \ref{sec:evaluation} provides further examples using the modal abstractions in proofs.
\paragraph{Temporal Operators}
The best known class of modal logics in the systems community is undoubtedly temporal logics (whether \textsc{LTL}~\cite{pnueli1977temporal}, \textsc{CTL}~\cite{emerson1982using}, \textsc{TLA}~\cite{lamport1994temporal}, or others),
due in part to Lamport's influence~\cite{lamport2002specifying} leading to its not-infrequent use in specifying
distributed algorithms \cite{ongaro2014search,ailijiang2019wpaxos}.
\paragraph{Nominalization}
Some classes of assertions benefit specifically from \emph{naming} the explicit conditions where they are true (as opposed to simply requiring them to be true \emph{somewhere} or \emph{everywhere} as in the most classic modal operators).
This naming generally resembles
the \emph{satisfaction} operator of 
\emph{hybrid logic}~\cite{blackburn1995hybrid,areces2001hybrid}: $@_\iota P$ which evaluates the truth of $P$ at the named (Kripke model) state $\iota$. For this reason we refer to the general idea of naming circumstances explicitly as \emph{nominalization},
even though the examples we discuss are not necessarily actually hybrid logics.

An example utilization of state naming explicitly on the assertion appears in program logics such as Iris~\cite{jung2018iris}, which enables encoding of usage protocols (e.g. state transition system) resembling typestate~\cite{strom1986typestate,garcia2014foundations} as specifications. Protocol assertions are \emph{annotated with the name of the last (abstract) state at which the protocol is ensured}.
One classic application of this idea to systems is 
Halpern et al.'s work adapting modal logics of
knowledge to deal with distributed systems~\cite{halpern1990knowledge,halpern1989modelling,halpern2017reasoning}.
In most of that line of work, $\mathcal{K}_a(P)$ indicates that the node $a$ in the system \emph{knows} or \emph{possesses knowledge of} $P$
(for example, a Raft node may ``know'' a lower bound on the commit index).
Alternatively, a modality $@_i(P)$ may represent that $P$ is true of/at the specific node $i$~\cite{gordon2019modal} (e.g., that node $i$ has stored a certain
piece of data to reliable storage).
These permit capturing specific concepts relevant to the correctness (and reasoning about correctness) of a certain class of systems,
involving facts about specific named entities in the system.
\\

In each of these cases, the fact that these facts are described using modalities with the core modal property $P \Rightarrow Q$, then $M(P) \Rightarrow M(Q)$ means, for example, that if a
process $p$ knows that the commit index is greater than 5 (e.g., $\mathcal{K}_p(\mathsf{commitIndex} > 5)$)
no extra work is required to conclude that the node knows it is greater than 3 (i.e., that $\mathcal{K}_p(\mathsf{commitIndex} > 3)$),
because this follows from standard properties of modal operators as described above.
In contrast, if verification instead used a custom assertion $\mathsf{minCommitIndex}(5)$ to represent the former knowledge,
one would need to separately provide custom reasoning to conclude $\mathsf{minCommitIndex}(3)$.

\subsection{Virtualization in Systems Software}
\label{sec:background}
One of the most common forms of abstraction provided by systems software is \emph{virtualization},
which abstracts the relationship between conceptual and physical computing resources. 
Operating System (OS) kernels virtualize memory locations and quantity (via virtual memory and paging~\cite{Denning1970VM}).
Distributed language runtimes may virtualize addresses, even when processes may migrate across machines~\cite{jul1988fine}.
Filesystems virtualize locations on disk~\cite{rodeh2013btrfs,hitz1994file,Rosenblum1992LFS,bonwick2003zettabyte}.
Programs built on top of the corresponding systems software layer work logically at the level of these virtualized resources,
and it makes sense to specify the systems software directly in terms of those abstractions.
\paragraph{Access by Translation}
Accessing virtualized resources via translation is a common way to virtualize notions of location (e.g.,
virtual memory addresses, inodes or object IDs instead of disk addresses~\cite{bonwick2003zettabyte,hitz1994file,Rosenblum1992LFS,rodeh2013btrfs}).
%Accessing the hardware resources in the presence of sharing means having more resource references (references to the virtualized resources) to access the \emph{mapped} resources that are physical and potentially fewer. 
B-trees, page tables, and related structures both behave like maps, when the corresponding physical resources exist as such
just in a different location.
Control over the lookup process (e.g, handling the case of a missing translation entry) allows for additional flexibility,
such as filling holes in sparse files, or demand paging (both from disk or lazily populating anonymous initially-zeroed mappings).
Although the realization of these maps may differ from system to system based on the context (and sometimes hardware details),
they are semantically --- logically --- partial maps, worth treating as such in verification.
\paragraph{Virtualization of Memory Locations}
\label{sec:backgroundonmachinemodel}
\todo{change the text}
\todo[inline,color=pink]{Based on the note I took above, I changed the text}
A typical general purpose computers make the memory resources in RAM as \emph{virtually} available. In other words, a program asking for a memory unit from an operating system kernel is served with a memory address that is \emph{virtualized} such that it may be \emph{mapped to} a physical RAM location or not. As our informal definition suggests, the common technique for \emph{virtualization} of memory locations in OS kernels relies \emph{translating}(\emph{mapping}) virtual addresses into physical ones. The conceptual address-translation map is implemented with processor's(CPU) page-table trees as shown in Figure \ref{fig:pagetables}. In this Figure, we see a typical 4 levels of page tables, a virtual address on the top indexing into the different levels of the page-table tree. Fabrication of virtual addresses out of limited physical addresses is provided with entries in the page-table tree where each points to a physical address aligned to 4KB (4096 byte) boundaries. An address translation requires traversal of a series of tables starting from level 4, and it is conventionally referred as a \emph{page-table walk}. The traversal ends in the level 1 page table with the final lookup to the actual page of physical memory holding the requested data, and the low-order 12 bits being used to index into this page.
\begin{figure}
    \includegraphics[width=\columnwidth]{pagetables2.png}
    \caption{x86-64 page table lookups.}
    \label{fig:pagetables}
\end{figure}


\todo[inline,color=cyan]{Ismail, is this text copy-pasted from the VMM paper? 
It has phrasing like ``In this paper...''
That's actually a major issue, self-plagiarism of another peer-reviewed piece of work can get
you into a lot of trouble.
}\todo{inline,color=pink}{I am well-aware what counts as as (including self-plagiarism). These were annotated with todos including cautions for changing the text.}
\paragraph{Virtual Memory Managers(VMM) with Multiple Address-Spaces}
\label{sec:backgroundonvmm}
\todo{change the text}
An intuition for the abstraction that \emph{contains} a page-table tree is called an \emph{address-space} of a process whose virtual addresses to physical addresses. A typical OS manages multiple processes at once . Each process is identified by an \emph{unique root address} of its page-table tree. As part of switching to a new process, i.e., loading the address-space mappings as the current view of the memory, the CPU stores the unique root address of the page-table tree in a specific register (\texttt{cr3}) as shown in Figure \ref{fig:pagetables}. Using different mappings, which map only disjoint portions of physical memory (with some exceptions in the next section) is how the OS ensures memory isolation between processes. Virtual memory managers are the modules in OS kernels handling this kind of tasks. The treat address-spaces as containers of virtual addresses so that they can bookkeep which which virtual addresses are valid (and in what way) in which address spaces.
\paragraph{Virtualization of Disk-Blocks in Filesystems}
Another important computer resource computer resource that is \emph{virtualized} for access is disk blocks. Filesystems are computer programs that serve to other programs for their disk resource need. Like VMMs using page-table trees to fabricate more memory addresses than what actually exists, filesystems uses indexing maps for fabricating more \emph{virtual disk-page} addresses which are conventionally called \emph{disk-pages} to be distinguished from physical \emph{disk-block} addresses.

Filesystems do not only handle the address-translation from a disk-page to a disk-block but also need to handle the different \emph{modes} of computing in which addressing disk resources occur at different consistency modes: a disk block persisted on the disk and a disk-page which may not be persisted on the disk yet. To handle the consistency of the disk resources in the existence of different modes of execution, filesystems employ different policies when indexing the disk resource. Which data-structure is going to be used to implement address-traslation from disk-pages to disk-blocks? What is going to be the \emph{update-policy} to the indexing map implementing address-translation which defines the \emph{consistency-policy} for the disk resources? Different filesystem implementations give different answers to these questions. One example answer to updating policy is copy-on-write (\textsf{CoW}). A (\textsf{CoW}) filesystem performs updates by copying modified data to \emph{new and unused} locations on disk rather than updating \emph{in-place}, ensuring that in the absence of physical storage corruption, there is \emph{always} a fully consistent filesystem on disk, removing the need for recovery from journals in the case of a crash, which can be very expensive for large volumes.
%\textsf{ModFS} is implemented in \textsc{Go}~\cite{donovan2015go}, using the FUSE~\cite{fuse} interface to run a filesystem in usermode. \textsf{ModFS}'s code is translated to \textsf{Roqc} using the \textsf{Goose}~\cite{chajed2019verifying,chajed2021gojournal} tool, which translates a significant subset of \textsf{Go} to \textsf{Roqc} to permit formal verification of \textsc{Go} code using \textsf{Iris}~\cite{jung2018iris}. \textsf{Goose} generates code in an extension of \textsf{Iris}'s \textsf{HeapLang} with libraries for disk manipulation and \textsc{Go} primitive types (e.g., maps) as shown in Figure \ref{fig:modfslayout}.

%On the left part of Figure \ref{fig:modfslayout}, we see the shaded region -- \textsf{ModFs} storage layer-- under the yellow layer --- the \textsf{ModFs} system call layer. 
%We have verified the parts of the shaded storage layer involved in creating a new on-disk snapshot, up to an axiomatized specification of of the \textsf{Bcache} component that implements a simple in-memory cache of disk blocks.
%This primarily involves the \textbf{write\_amplification} function on btree nodes (from the \textsf{Btree} module), and its associated dependencies, including the \textbf{split}, \textbf{put}, and \textbf{del}(ete) operations on nodes, and the \textbf{write} serialization function.
%This also includes snapshot operations including \textbf{snap\_initialize} and \textbf{snap\_alloc} functions (among others) in the \textsf{Snapshot} module, and freelist operations (\textbf{free} and \textbf{allocate}) from \textsf{Freelist}, as well as the supporting functions from the \textsf{Chunk} (roughly a structured disk block abstraction), \textsf{PageMap} (raw disk block abstraction), \textsf{and Lock} (thread synchronization via a global lock map) modules.
%\textsf{Storage} (a data structure of runtime pointers\colinsays{doesn't really explain it, I don't know what this component is/does}), \textsf{BtrItr} (an iterator library), \textsf{inode}, and \textsf{Btree} have not yet been fully verified, though the components involved in write amplification have been verified. The filesystem does run and standard filesystem functionality (core file and directory operations) work.
%This paper focuses on the verification of write amplification, which is verified up to a small number of axioms for \textsf{Go} data structures\colinsays{update w/ very short summary of remaining admits} and the unverified \textsf{Bcache} specification.
%\colinsaysinl{How many lines of code are covered by the verification? (excluding the bcache) This might help assuage reviewer concerns that we're not done, by making the scope of this project more explicit so ``why didn't you finish'' has a self-evident answer of ``because they already did a ton of work''}

%\textsf{ModFs} is a \textsf{CoW} file system where the mapping of the logical locations, e.g., file pages indexed by file offsets, to disk locations by employing a \textsf{CoW} tree \cite{Astrahan1976SystemR,shadowclone} structure. In Figure \ref{fig:cow_index_tree}, we see that accessing and updating the $\textsf{page}_n$ of file \textsf{"foo"} referred by the file offset $\textsf{offset}$ 
%\[ \textsf{write}(\textit{foo},\textsf{offset})\]
%requires locating the corresponding physical disk block of the in-memory $\textsf{page}_n$ whose mapping is establised the \textit{copy-on-write} index tree (in the middle layer). Like Btrfs ~\cite{}, \textsf{ModFs} indexes all filesystem objects (e.g. files and directories) as a nested trees, and to map a logical page to a physical page, the file system first locates the root of the nested index-tree belonging to \textsf{"foo"}.
Semantically, in \textsf{ModFS}~\cite{} (as in other \textsf{CoW} filesystems~\cite{young1992episode,bonwick2003zettabyte,rodeh2013btrfs,Rosenblum1992LFS,hitz1994file}) any update to file data or metadata is realized on  unused disk locations by treating the tree as a persistent data structure. 
As an example, Figure \ref{fig:cowindex} shows that an update to the file $\textsf{"foo"}$ at $\textsf{"offset"}$ ends up with locating the related disk chunk(multiple disk-pages) in the gray region, copying it to newly allocated disk chunks (dotted chunks on the right side of the bold long-dash-point line of the middle layer withing dotted section), and performing the update on these newly-allocated disk blocks. This requires further updates to those metadata blocks which previously referred to the old block locations to instead reference the newly-allocated blocks. This must be repeated until the allocation/update of the new chunks percolates up to the root (including the root), through a process called \textsf{\textit{write-amplification}}.

\begin{figure}[t]
    \includegraphics[width=0.75\columnwidth]{cow_index.png}
    \caption{A Filesystem called \textsf{Modfs} Indexing its Physical Pages for File Pages with CoW (copy-on-write) Tree}
    \label{fig:cowindex}
\end{figure}

Like VMMs navigating between different address-spaces, a \textsf{CoW} filesystem switches from either the old to the new root yields consistent views of the filesystem at different points in time, each commonly referred to as a \emph{snapshot}. Like VMMs loading a process's memory mappings to be the current view of the memory by setting the control register to show the unique root address of the page-table tree of the process, the filesystem to atomically switch between consistent on-disk states, by updating the oldest so-called \emph{super} chunk at a fixed location on disk. Super chunks, as we see in Figure \ref{fig:cowindex} as ($\textsf{Super}_0$ and $\textsf{Super}_1$), are the first two blocks of the disk dedicated to keep the state of the file system which is composed of some meta-data on the filesystem, and most importantly to the root chunk of the file system index-tree. 
\begin{figure}
    \includegraphics[width=\columnwidth]{journalling.png}
    \caption{A Filesystem Utilizing \emph{Journalling} for Consistent Disk-Layout ~\cite{chajed2021gojournal}}
    \label{fig:journal}
\end{figure}

\paragraph{Different Models in Ensuring Disk Consistency of Filesystems}
\todo[inline,color=cyan]{This looks like it's verbatim copy-pasted from the ModFS draft; if this exact text
  it used in this publication, it means we'd need to completely rewrite the relevant text in that draft.}
Therefore, in case of a crash during the update to the \textsf{file "foo"}, we can reboot the file system from the previous persisted root, \textsf{root chunk}, via reading the $\textsf{Super}_0$. This means the whole disk consistency checks are unnecessary when recovering from a crash~\cite{bonwick2003zettabyte,hitz1994file}; recovery is immediate, as opposed to requiring hours or days~\cite{henson2006reducing,agrawal2009speedy} (or longer~\cite{henson2006chunkfs}) for recovering a large volumes consistency from logs. 
An important additional subtlety with this kind of filesystem is \emph{when} blocks in the reclaimed region can actually be reused: only \emph{after} the snapshot that made those blocks ``free'' has been completely written to disk. Allowing reuse before this could result in in-place updates: part of the new snapshot could attempt to store data in space used by the prior snapshot, allowing inconsistent on-disk structures.

With their intricate semantics and critical place in modern systems, formal verification of functional correctness for storage systems (including filesystems and other kinds of storage) have drawn much attention~\cite{ntzik-gardner-15,ntzik2018concurrent,chen-crash-16}\todo{lots more cites}. Unlike our \textsc{CoW} filesystem \textsf{ModFs} which allocates new chunks and rewrites all the nodes from leaf to root on the write-amplification path, some filesystems allows in-place updates to handle the heavy workloads more efficiently in principle. Unlike \textsc{CoW} filesystems, filesystems with in-place updates need to implement some form of \emph{journalling} to ensure that a crash partway through an in-place update can be rolled back (using an undo journal that records the original contents before modification as abstarcted in Figure \ref{fig:journal}) or completed (using a write-ahead journal that first writes new data in unused space before \emph{also} updating in place).\footnote{\textsf{CoW} filesystems can be viewed as a twist on write-ahead journals that uses the journal as the actual data once written to a fresh location, thereby reclaiming the original location. This results in a performance and complexity trade-off: journalling writes twice for every updated block and requires recovery logic to roll back or re-execute the log, while \textsf{CoW} techniques write each update only once and require slightly more intricate allocation strategies to compact fragmentation.}

Essentially, successfully writing the new superblock for a new snapshot is analagous to completing a valid journal entry, except it \emph{also} has the effect of updating the live filesystem data due to making the ``journal entries'' valid filesystem data and metadata blocks (an idea that goes back to early work on log-structured filesystems~\cite{Rosenblum1992LFS}).
In fact, we can think of the atomicity of the recovery from the crash state conceptually equal to the single block disk write persisting the change from $\textsf{Super}_0$ to $\textsf{Super}_1$ in Figure \ref{fig:cow_index}, therefore, considering \textsc{CoW} semantics, we do not need to consider the specification and proof of crash condition separately; instead a global never-violated invariant (not even temporarily broken) of the filesystem ensures the on-disk state is always consistent.
\paragraph{Memory Reclamation in Client Code} In a typical configuration, allocating, deallocating and referencing memory locations for a client code leave the memory layout in a form of DAG (directed-asyclic-graph) with a unique root which resembles the first stack pointer referencing the allocated memory. To ease picturing the memory layout after a set of requests from the client code (for allocation, referencing etc.), we can think of the unique roots of DAGs as \emph{super} block addresses -- this time multiple ones, not only two -- in Figure \ref{fig:cow_index}, from which a path is created for a stack pointer reference to the allocated memory. To reclaim the allocated memory location that is no longer being accessed, systems uses a well-known method called \emph{reference-counting} in which a new reference to the allocated memory increases the reference counter on it w.r.t a certain regime. 
\paragraph{Consistency of Reclaimed Memory} A sound memory reclamation is based on the conditions where the unique is not \emph{shared}, and there is no \emph{reachability} from the root to any the references pointing to the allocated memory. Once the requirements are satisfied then the memory location can be reclaimed \emph{soundly} -- i.e., without violating any access through a reference to it.
\paragraph{Buffering} Buffering is a technique used in variety of systems (filesystem page caching, write buffer in multi-processor architectures, translation-lookaside-buffers for translated addresses etc.) to delay the costly operation (committing file pages, flushing buffered writes from write-cache to memory etc.) making updates consistent.
\paragraph{Consistency of Reads of Buffered Writes} Consistency of reads of any value potentially buffered depends on the semantics of the operation making them \emph{observable}. A typical architecture such as Intel-x86 considers the \emph{flush} of buffered writes \emph{non-deterministically} -- i.e., at any time at any number of buffered writes. On the other hand, a typical filesystem can follow a certain policy to commit the in-memory file pages to the disk.   

%\section{Resources in Systems Software}
%\label{sec:systemsoft}



%\subsection{Speculation in Consistency} \todo{speak about the weak-mem modalities in here.}
%\subsection{Sharing} Sharing is one of the fundamental reasons for kernels to {virtualize} resources,
%for creating the illusion of exclusive access to virtualized versions of shared resources, including CPU time (via preemption).
%An implementation of sharing relies on two concepts: \emph{indirection} and \emph{coordination}. Indirection is a method that allows the mapping of resources of one type into another. By doing so, it allows treating the limited resources (e.g. hardware resources such as physical memory) as if there are more than they actually are (e.g. via virtual memory addresses and paging). Consequently, virtualization for the sake of ensuring availability requires \emph{sharing} of the resources that exist in reality.
%%\paragraph{Isolation Hand-in-Hand with Sharing}
%%\Ismail{put a few words on abstractions such as address space, stack region, snapshots ,etc.}
%%\Colin{Not sure what you want here?}

\todo[inline]{Colin says: the nominals section needs another shift for the new audience:
You don't need to define modalities here, but do need to give initially the classic notion of nominal
(name for a Kripke world) before bridging the gap to using the concept of naming
for something a little different.
}

\section{Contingency Decomposition of a System}
In this section, we argue the position that \emph{modalities}  
should be a go-to approach when specifying and verifying low-level systems code.
We explain how the concept of a \emph{resource context} helps guide the design of new modalities
for verification of systems code, and
we justify our perspective by discussing prior systems that have used modalities for systems verification
successfully, arguing that they fit into the verification design pattern we articulate,
and explaining how this approach might apply to other systems verification challenges.

To explain our ideas in the general systems understanding, we briefly recap some of the background and themes our ideas build on again, casting them in a certain way to bring out the relevance of our philosophy.
  
\todo[inline]{Colin says: I don't think this section needs to change much,
but it should be read carefully with the new audience in mind,
possibly giving more details on what systems concepts each system is dealing with, for a non-systems audience.}
\paragraph{Shallow Modalities in Iris Separation Logic}
\label{sec:modal_in_iris}
\todo[inline,color=cyan]{this also seems taken from the VMM work}
Classically modal operators are given direct semantics in terms of an underlying model, which for \textsf{Iris} would be a collection of \textsc{CMRA}s; this is the case for \textsf{Iris}'s one built-in modality for step-indexing~\cite{krebbers2017essence}. However in general, many modalities can be given proof-theoretic semantics directly within a logic. This is the case for \textsf{Iris}'s weakest precondition modality (which is defined within the \textsf{Iris} logic itself), as well as for a wide range of other modalities in the literature~\cite{restall2002introduction}. These modalities are typically multi-modal, with a form of roughly $M~\overline{ctxt}~\phi\triangleq\ldots\rightarrow \phi~\overline{ctxt}$. In a substructural setting~\cite{restall1993modalities,krebbers2017essence} the implication $\rightarrow$ is typically replaced with substructural implication $\sepimp$. 

This definition also reveals a subtlety about the denotation of assertions $\phi$ --- they are effectively \emph{predicates} of some kind, functions from some possible world into some logical algebra such as a BI-algebra~\cite{ohearn1999bunched}. Taking this interpretation literally gives rise to a convenient way to embed new modalities in \textsf{Iris} in a way well-suited to our use cases. If $B$ is a type with a BI-algebra structure, then any space of predicate functions (e.g., $\mathsf{val}\rightarrow B$ as a simplification) also carries a BI-algebra structure, lifted from $B$, often called the \emph{pointwise lifting} of the algebra. We will exploit this repeatedly, lifting the BI-algebra structure from \textsf{Iris} itself to \textsf{Iris} predicates used as assertions within our own modalities. 
Connecting this to \textsf{Iris}'s typeclasses for interactive proofs~\cite{krebbers-17} then permits working within the embedded logic as in \textsf{Iris}.

In addition, the modal definitions themselves may represent ownership of resources in a substructural setting~\cite{restall1993modalities,dovsen1992modal,kamide2002kripke}. This will allow our modalities to simultaneously represent ownership of certain resources used for interpreting the modality itself, as well as whatever resource ownership is implied by the modal argument.

\subsection{Decomposing a System into its Constituents \emph{Contingently}}
\label{sec:definitions}
\begin{table*}[t]
\centering
\caption{Modal Decomposition of Program-Logics.}
\label{table:decomposition}
 %$\{\Diamond (P) \}$ write  $\{ P\}_{n}$
%$\{\overset{ICut^{n}}{\hookrightarrow} (P) \}$ return $\{ P\}_{n} $
\begin{tabular}{@{}lcccp{4cm}@{}}
  \toprule
{\scriptsize Modality}& {\scriptsize Context} & {\scriptsize Elements}  &  {\scriptsize Nominalization} & {\scriptsize Context Steps} \\ \midrule
\scriptsize Post-Crash~\cite{tejthesis,perennialgit,tejperennial19} & \scriptsize $\Diamond \; P $  & \scriptsize $  \ell \mapsto_{n}^{\overline{\gamma}} v $ & \scriptsize Strong &  \scriptsize Crash Recovery    \\ 
%GC-Modality &      & \multicolumn{2}{c}{\multirow{2}{*}{Because why not}} & 8                                   & & ~\cite{}\\
\scriptsize NextGen~\cite{larsnextgen25} &  \scriptsize $\overset{t}{\hookrightarrow} \; P$  &\scriptsize  Own (t(a))  & \scriptsize Strong & \scriptsize Determined Based on the Model$^{*}$   \\
\scriptsize StackRegion$^{*}$  ~\cite{larsnextgen25} & \scriptsize $\overset{ICut^{n}}{\hookrightarrow} \; P$      & \scriptsize $\fbox{n} \; \ell \mapsto v$                                 & \scriptsize Strong & \scriptsize Alloc and Return to/from stack\\
\scriptsize Memory-Fence~\cite{fsl,fsl++,derekrustbelt20} & \scriptsize $\triangle_{\pi}$ and $\triangledown_{\pi}$      & \scriptsize $\ell \mapsto v$                                 & \scriptsize Weak & \scriptsize Fence Acquire and Release  \\
\scriptsize Address-Space~\cite{kuru2024modalabstractionsvirtualizingmemory} & \scriptsize [r]P     &  \scriptsize $\ell \mapsto v$ & \scriptsize Weak                         & \scriptsize Address-Space Switch  \\ 
\scriptsize Ref-Count~\cite{amalreal2024}&\scriptsize @$_{\ell}$ P    & \scriptsize $\ell_1 \mapsto v$ & \scriptsize Weak  & \scriptsize Allocating, Dropping and Sharing a Reference \\ \bottomrule
\end{tabular}

{\scriptsize *The StackRegion Modality is an instance of NextGen (called the Independence Modality in \cite{larsnextgen25})}.
%and the $\Diamond$ Figure for Post-Crash Modality uses notationis taken from NextGen Modality paper ~\cite{larsnextgen25}.
\end{table*}

Many existing program logics for system verifications have a common structure, which maps to modalities with a couple extra dimensions of design.
We summarize our discussion of these logics in Table \ref{table:decomposition}.
We discuss, based on examples, common aspects of how we intuitively think about correctness of systems code in many contexts,
articulate those pieces, and call out the commonalities across a range of systems.
\subsection{Resource}
Consider first the address-space abstraction in an OS kernel. An address-space of a process is a \textsf{container} of \textsf{virtual} addresses referencing data in memory. One would expect to have \textit{points-to} assertion from separation logic to specify \textit{ownership} of a memory reference pointing to some data. But that ownership is relative to a specific
address space --- a specific container. We tend to think directly about what is true \emph{in an address space},
with the simplest piece being an association between a virtual address and the data it points to.
We call the simplest piece, in this and other examples, the \emph{resource element}:

\begin{definition}[Resource Elements]
%Inherited from separation logic, we identify the resource element as the indivisible logical element on which contingency defined by the modality holds. 
The simplest atomic facts we want to work with in a particular setting, specific to that setting.
\end{definition}
By definition, the resource elements are specific to some limited domain or setting.
For example, knowing that a certain address points to a 32-bit signed integer representing 3 is knowledge restricted to a certain address space.
In general,  we call these domains that any resource element is tied to \emph{resource contexts}:
\begin{definition}[Resource Context]
A resource context is an abstraction, context, or container of resource elements of the same type, e.g., an address space of a process.
\end{definition}
We discuss a range of examples for each of these in turn.

Table \ref{table:decomposition} gives additional examples of systems and their corresponding resource elements and contexts where these elements reside, though none of the work in that table analyzes itself according to the structure we are giving.

Except for Post-Crash-Modality, one can think of the resource contexts in the first column in Table \ref{table:decomposition} as containers for the corresponding resource elements in the second column.

\paragraph{Stack Regions} When reasoning about stack frame contents, the resource element would be a stack-memory points-to assertion ( $\fbox{n} \; \ell \mapsto v$)
indicating that a certain offset into stack region $n$ holds value $v$. 
%which is an instance of ownership assertion ($\textsf{Own(t(a))}$) for which a well-behaved transformation ($\textsf{t}$), e.g. $\textsf{ICut}^{n}$ algebra quantifying over valid stack region ids. 
\paragraph{Virtual Memory} For virtual memory management, a \textit{virtual-points-to} ownership assertion pairing a virtual address ($\ell$) with data ($v$) in an address space is natural. A process's address space with the root address \textsf{r} is an abstraction that is treated as a container for virtual address mappings, $\ell \mapsto v$;

\paragraph{Weak-Memory} When considering weak memory models, we also want points-to information (address-value mappings).

\paragraph{Reference Counting} When dealing with reference-counting APIs, we may care to specify reachability of memory nodes ($\ell \mapsto v$) in a certain context defined by a shared root address. A shared memory address $\ell$ can be the root of the graph that can be a container of memory nodes ($\ell_1 \mapsto v$) that are reachable from the root $\ell$. 

\paragraph{Post-Crash}
%\Colin{Ismail, not sure how to describe this. We don't want to go into the weeds on the NextGen}
%that can be jumped from a shared memory referenced whose reference count is at least 1 are also example resource elements from memory management task at different abstraction levels. 
The resource element of Post-Crash Modality is not obvious in Table \ref{table:decomposition}, and needs a bit of explanation. Perennial, based on the Iris logic, has both disk-points-to assertions $d[\textsf{p}] \mapsto_{n} v$ (for a specific disk $d$) and in-memory points-to assertions $\ell \mapsto_{n}^{\overline{\gamma}}$. Perennial crash-recovery logic book-keeps resource names (can be thought of as logical variables) $\overline{\gamma}$ to identify 
which assertions (resource elements) remain valid after a crash --- these assertions
are only usable while the names in $\overline{\gamma}$ are valid, and a crash resets
them, discarding assumptions about volatile state. A subtlety of the notion of a resource context is that, unlike the earlier examples, the context does not need to be a literal data structure. It can instead be (various forms of) a set of executions, as in the Post-Crash and NextGen modalities.
The Post-Crash modality $\diamond$ expresses that the assertion $P$ will be true after a crash discards
all unstable storage (i.e., RAM).
This was the inspiration for the NextGen modality, which is in fact a framework for defining ``after-$t$'' modalities, where $t$ is an transformation of the global state.\footnote{The transformations are subject to some
technical constraints that are unrelated to our point here.}
%whose capability is considered to be relevant to be kept in the crash invariant $\fbox{C}_n$, and to be used when a crash occurs.\Colin{Ismail, not sure about last line above. Can you better explain gamma?}
%regions of state (including auxiliary/ghost state) which persist after a crash (anything not identified 

%Perennial keeps the consistent state of persistent storage in a crash invariant at : multiple disk-points-to assertions $disk[\textsf{p}] \mapsto_{n} v$ in the \emph{crash} invariant $\fbox{C}_n$ became consistent at the n$^{\textsf{th}}$ successful completion of writes, i.e., generation number, from in-memory to disk. The in-memory updates ($p \mapsto_{n} v$) are also indexed with the generation number $n$. Since Perennial is built on top of Iris Separation Logic which implicitly decorates ownership assertions -- both disk and in-memory points-to assertions -- with ghost names ($\gamma$), Perennial utilizes this and bookkeeps points-to assertions ($\ell \mapsto_{n}^{\overline{\gamma}}$) of a certain version number $n$ as \emph{a resource element lifted by resource names} needed in case of a crash to distinguish the resources to be invalidated, i.e., discard the piece of the physical state that resides in the memory and preserve the resources that reside in the persistent disk. 

%\Colin{I don't understand this next bit, we need to discuss:}
%\begin{remark}[Resource Element Names]
%Regarding the names, like Iris resource names, all the resource elements in the second column have names; they are not subject to change --- \emph{static} --- when they are not parameterized by names and mentioned explicitly in the element when they do not matter for the contingent truth.
%\end{remark}
%\Colin{This in particular would benefit from an earlier intuition discussion}

%\section{Resource Contexts}
%
%\looseness=-1
%
%
%
%\begin{definition}[Transforming Resource Elements]
%    Any resource element that is parameterized w.r.t to a parameter such as resource \emph{names} $\overline{\gamma}$ in Post-Crash Modality is subject to the change, and this change has to be acknowledged in the resource context.
%\end{definition}
% Since we have the resource elements of Post-Crash modality parameterized by names as shown in Table \ref{table:decomposition}, the modality has to acknowledge how the resource names plugged into the resource elements relate to the names that already exist globally --- Perennial utilizes this for invalidating the points-to in-memory assertions whose names are not part of the plugged-in resource names. To do so, Post-Crash Modality resource context $\Diamond$ has a relation (ignoring the universally quantified machine states Perennial needs as it utilizes $\Diamond$ for the crash-aware Hoare triple not client specification/proof) with universally quantified new names, $\overline{\gamma'}$, to be related to the old global names and plugged into the parameterized resource elements to obtain post-crash values of the elements $\lambda \; \overline{\gamma} \ldotp P $ via disjunctive choice on the new and old values presented by the resource-element transform relation ($R$)
%\[\Diamond \; P \triangleq  \forall_{\overline{\gamma'}} \ldotp R(\_,\overline{\gamma},\overline{\gamma'}) -\ast P (\overline{\gamma'}) \ast R(\_,\overline{\gamma},\overline{\gamma'})\]
%This, in fact, inspires NextGen Modality to encode resource-element transform relation as a resource algebra, and make the resource element generalized w.r.t. a \emph{well-behaved} transformations defined by the rules of resource algebra. 
%\paragraph{Dynamically Naming Resources} Names are global and each resource element has a name. Unlike others in Table \ref{table:decomposition} whose resources are fixed (inspired by Iris, each points-to relation has a hidden name  $\gamma$), Post-Condition Modality resource element is parameterized by the underlying resources' names. Unlike fixed %naming, dynamic naming in which resource elements are parameterized by the resource names, the knowledge of which resource name is transforming because an
%any other resource elements named with the ones th
%\section{Resource Ambiance}
%While the central aspects of using a modality to capture contextual reasoning are the elements and contexts, there is \emph{sometimes} more to it.
%Sometimes there are multiple modes of execution (e.g., crashing, running in a particular address space),
%so not only state descriptions but also rules of the program logic itself must be decorated with
%a \emph{resource ambiance}:
%\begin{definition}[Resource Ambiance]
%Resource ambiance is a dependency of the logic's description of program behaviour on
%a particular mode of execution tied to the modality.
%\emph{general form of contingent} status of resource elements of the same type against their resource context. The crucial aspect is to understand how \emph{identifiable} both the context and its elements should be: \emph{nominalization} of the contex%t and its elements in the ambient logic, e.g., how should stack-points-to relations together with the StackRegion Modality should be \emph{referred} in a Hoare triple for Return step in the ambient separation logic.
%\end{definition}
%In formal presentations, this typically shows up with modified Hoare triples $\{P\}\;C\;\{Q\}_A$,
%where $P$, $C$, and $Q$ are the precondition, code, and postcondition that code satisfies assuming that precondition (as before) and $A$ is essentially a marker of how the reasoning about the program interacts with the modality --- such as a post-crash assertion, or an indication of what address space the verification is assuming.
%This means the proof rules for deciding how program operations affect knowledge of program state
%can depend on, restrict, or manipulate the context assumptions of reasoning about program behaviour, not just
%descriptions of program state at some moment in time.
%Not all use cases for modal assertions require an ambiance, only those where the modality corresponds to modes of execution.
%
\subsection{Nominals for Systems Resource Contexts}
Finally, another design point is the question of whether or not resource element assertions must explicitly track
their corresponding context, or if they implicitly pick up their context from where they are used.

\emph{Strong nominalization} is the case where resource elements must explicitly include
the identity of their intended context, while \emph{weak nominalization} occurs when the resource
elements implicitly pick up the relevant context from how they are used.
The first three modalities in Table \ref{table:decomposition} are strongly nominalized,
with the resource elements generally carrying identifiers of a specific modality usage.\footnote{The Post-Crash modality does not look like this in the presentation here; technically the definition of $\diamond$ quantifies over names $\overline{\gamma}$ internally, dealing with sets of possible contexts.}
The last three are weakly nominalized.

This choice trades off complexity against flexibility and scoping constraints.
Strongly nominalized elements track additional specifier/prover-visible book-keeping data.
But in exchange for carrying those identifiers of their context with them, 
strongly nominalized elements can be used together under any modality. For example, one can
use the StackRegion modality to talk about two different stack frames simultaneously for code
which accesses multiple stack frames: $\fbox{n} \; \ell \mapsto v \ast \fbox{n+1} \; \ell' \mapsto v'$.
Using a given strongly nominalized assertion element under different modalities for different
frames does not change its meaning.

By contrast, weakly nominalized elements are more concise, but then make talking about multiple 
contexts together marginally more complex: changing which modality an assertion is used with drastically changes its meaning. In the case of the Ref-Count modality, $@_\ell(\ell_1\mapsto v)$
says that $\ell$ points to a reference count wrapping $\ell_1$, while placing the $\ell_1\mapsto v$ under a jump modality for a different location entails talking about a different region of memory.

In general, use cases where code frequently manipulates small parts of multiple contexts together
should prefer strong nominalization, while use cases where usually larger portions of a single context
are at issue should prefer weak nominalization.
\section{Universal Properties over Resource Contexts with Nominals:  An Filesystem Recursive Trees Structure}
\label{sec:overviewnode}
Up until now, we discuss the concept of resource and resource context related under a certain \emph{nominalization} scheme. These definitions establishes the foundation for local properties of modal patterns. In this section, we discuss how we would be discussing the universal properties over the modal context themselves through an filesystem tree example. By doing so, we are able to show how the local properties defined over a resource and its context can be interfaced and exposed universally through reasoning over resource contexts. 

To reason about states of trees and subtrees in particular, we work with a modality for stating an assertion that is true \emph{of all nodes within a tree}, identifying the tree by a root note. We refer to the resources that provide the context of these modal assertions as the \emph{modal context}; for a recursively defined tree like those in \textsf{Modfs}, these contexts are recursively defined in terms of the tree structure.

%We define the modal context as the resources (e.g. a btree node tree, free page id list) which are accessed under some relation defined within the modal definition (e.g. being children of a cursor node). The choice of modal context can be structurally dependent on the semantics, in our case \textsc{CoW} semantics.

Write amplification traverses the overall tree depth-first, transforming the tree from the bottom up, one full subtree at a time. Concepually this means the amplification code's proof must track the fact that at any point in time, there are regions of the tree where each node has already been amplified (e.g., $\phi_{\textsf{amplified}}$ definitely holds for that node), and regions where nodes may or may not require amplification, and this will not be known until those nodes are visited (i.e., some basic validity $\phi_{\textsf{valid}}$ is known to hold, but not the stronger $\phi_{\textsf{amplified}}$).

%\setlength\intextsep{0pt}
\begin{wrapfigure}{r}{0.25\textwidth}
  \centering  \includegraphics[width=0.25\textwidth]{entail.png}
  \caption{Understanding the level of $\varphi$ satisfaction in the \textsc{CoW} tree of modal nodes}
   \label{fig:nodeentail} 
\end{wrapfigure}
We use a modality similar to $\text{\faSitemap}@[\ell]\phi$ to indicate that $\phi$ is true of the tree node rooted at $\ell$, as well as at \emph{all reachable children} of that node.
This means that establishing a particular $\phi$ throughout a tree must be done in a bottom-up manner (see Figure \ref{fig:nodeentail}).
Capturing this modally, rather than with more typical ad-hoc hand-crafted invariants, offers advantages in a number of proof steps, in addition to concisely capturing intuitive reasoning about subtrees with all nodes satisfying a particular property. One basic example is that while the actual disk sync code requires the full filesystem tree to be in an amplified state (hence write amplification), most of the filesystem works with more general trees. Since $\phi_{\textsf{amplified}}\vdash\phi_{\textsf{valid}}$, using our \faSitemap modality we can conclude $\text{\faSitemap}@[\ell]\phi_{\textsf{amplified}}\vdash\text{\faSitemap}@[\ell]\phi_{\textsf{valid}}$ by an analogue of Axiom K.
There are also other places in the code where a subtree satisfying a stronger assertion at all nodes must be linked into a larger tree satisfying only a weaker assertion overall: predicate representing the valid tree as the tree of nodes which are backed by resources allocated pages, i.e. $\phi_{\textsf{valid}}$, is weaker than the predicate representing the tree of nodes being currently amplified in the current snapshot. Again, the same reasoning principle makes this straightforward. As alluded to earlier, our $\phi_{\textsf{amplified}}$ and company are in fact \emph{predicates} parameterized by a choice of node, though by lifing \textsf{Iris}'s BI structure they can use all of the standard separation logic connectives. 
%\colinsaysinl{I cut some text here about the three-way split (commented out) that I wasn't sure how to use, but maybe I deleted something important?}
%To put it concretely, as shown in Figure \ref{fig:cow_index_tree.JPG}, \textsc{CoW} semantics determine what we can claim on the structure of the filesystem indexing tree:
%\begin{itemize}
%\item The bottom-up direction splits the tree into the two half at the cursor node: 1. the bottom part accessible from the cursor node and per-node-amplification is reflected in case needed 2. the upper part on which amplifcation has not reflected yet. 
%\item Horizontally, amplifcation path splits the bottom part of the tree into three different contexts: untouched tree nodes, nodes on the amplification path, and the ones generated as part of node splitting. 
%\end{itemize}
%Based on these two observations, in fact, bottom-up amplification determines the choice of modal context by giving the boundries of \textit{any claimable knowledge} (e.g. $\varphi_{\textsf{amplified}}$, $\varphi_{\textsf{untouched}}$): a subtree bounded by the cursor node.
%$\paragraph{Strong Nominalization} Some resource elements require strong \emph{association} with their contexts as \emph{up-to} proper identification of resource context. That means, not only does the resource context have to be identified with a nominal, but also resource elements need to be associated with some nominals that are related to the context nominal, because certain actions in operational semantics require \emph{identification} of the resource context and elements together. For a stack with $m$ regions, returning from a i$^{th}$ index roughly (ignoring the evaluation context issues related to the underlying logic):
%\[
%\infer{\textsf{wp} \; return \; cont^{i}(K') \{Q\}_n}{%
%\begin{array}{l}
%    \textsf{ExistsRegion} \; m  \quad\quad   n <= m - i  \quad\quad i <= m  \\
%\textsf{ExistsRegion} \; (m-i) -\ast \; \overset{.}{\hookrightarrow}^{m-i} (\textsf{wp} \; K'[shift \; i] \;\{Q\}_n)  \end{array}
%}
%\]
%asserting that with proper stack region lower bound (n), and the index falling to the proper range, knowing that the next required precondition ($(\textsf{wp} \; K'[shift \; i] \;\{Q\}_n)$) is independent of any stack points-to ($\fbox{m-i} \;\ell \mapsto v$) above the region $m-i$ by the StackRegion Modality (as an instance of NextGen Modality)  $\overset{.}{\hookrightarrow}^{m-i}$ (Independence Modality ~\cite{larsnextgen25}). Here, we see that both the resource context (Region Modality) and the resource elements (stack points-to assertions) nominalized, and they are aware of each other. Regarding the Post-Crash Modality, since Perennial utilizes it for extending Hoare triple encoding of Perennial with the crash recovery ( \textsf{wpc} ~\cite{tejperennial19, tejthesis}) such that in case a crash occurs on a disk-access, Hoare triple decorated with the crash recovery condition utilizes the capabilities in the crash invariant for the n$^{th}$ successful disk state (what NextGen ~\cite{larsnextgen25} calls \emph{generation} number). Since Post-Crash Modality is parameterized with the resource names ($\overline{\gamma}$), based on these resource names, the in-memory updates ($p \mapsto^{\gamma}_n v$) are ignored and only relevant successful disk updates ($d[p] \mapsto^{\gamma'}_n v$) are considered to create the consistent crash invariant n+1, $\fbox{C}_{n+1}$. Since NextGen Modality ~\cite{larsnextgen25} generalizes Perennial, lifting Post-Crash Modality into the client specification would require \emph{strong nominalization} as the proof would require the guarantees similar to the StackRegion Modality (Independence Modality ~\cite{larsnextgen25}) asserting the updates on disk are \emph{independent} of any in-memory updates.
%
%%When we look at the third column, we see the modal program logics that require strong normalization. Thinking of in-memory updates to be persistent on the disk, Perennial ~\cite{tejperennial19} decorates the in-memory ownership capabilities, which is resource elements of the  (e.g. $p \mapsto_{n} v_0$) with generation numberbrs 
%%For example, the resource elements in the separation logic utilizing \textsf{NextGen} modality and the modality itself are subject to strong normalization: both weakest-precondition and the ownership assertions are nominalized with region index such that the ownerships are valid as long as the constraint on the region index is satisfied -- e.g., all the ownership assertions with region index greater than the one on the weakest-precondition are valid against a crash-step in a filesystem or garbage collecting step in another system updating the consistent disk or reclaimed memory layout. \todo[inline]{maybe paraphrase more on referring them in the ambient logic}.
%
%\paragraph{Weak Context Nominalization} Unlike the resources we discussed so far, some resource elements are \emph{agnostic} to the resource context they belong. That means the resource context needs a nominalization as it needs to be distinguished from other resource contexts when certain actions in the operational semantics are taken (e.g., address-space switch in Table \ref{table:decomposition}), but the resource elements of the context are unaware of this context nominalization, and they may even not be labeled with any other nominal as well. For example, virtual memory references in an address space do not know which address space they belong, but the address spaces are distinguished w.r.t. the unique memory address of the root page-table tree therefore, a virtual-address-points-to ownership assertion does not need to be aware (or nominalized with) of the nominal of the address-space which is the unique memory address of the address table. Logically, specifying an address-space switch ~\cite{kuru2024modalabstractionsvirtualizingmemory} from the address space with the root address $r2$ to the one with the root address $r1$
%\[
%\{[r1] P \ast Q \} cr3 := r1 \{P \ast [r2]Q\}_{\textsf{cr3}}
%\]
%requires having the resource elements to be loaded into the memory 
%\[P \triangleq \ell_1 \mapsto v ...\]
%to be in the resource context of the address-space modality with the nominal $r1$, and, once the new address space is loaded --- i.e., cr3 is loaded with the other address-space root address ---, the resource elements ($Q$) that were loaded in the previous view of the memory are now introduced to the resource context with the nominal $r2$. Unlike what we discuss in the strong normalization (e.g. StackModality in Return specification) in which nominalized resource elements (stack points-to assertions) are guarded by Independence Modality ~\cite{larsnextgen25} (StackModality row in Table \ref{table:decomposition}) to preserve the soundness of the proof rule. However, with weak-nominalization, resource elements don't have nominals (Address-Space Modality Resource Element Column in Table \ref{table:decomposition}), and most importantly, address-space resource context switch, unlike shifted stack region in Return Specification, does not require any explicit independence check because all the resource elements at any time are either in one of two resource contexts or hosted in the ambient logic.

%\begin{definition}[Transforming Resource Context in Resource Ambience]
%    For the examples in this paper, a Resource ambiance is always a Hoare Triple indexed with the resource context nominals, which hosts the resource contexts. Unlike the transformation of resource elements hosted by a resource context (\textbf{Definition 3.3.}) handled by a relation in the modality definition, e.g. the resource element transformation relation in Post-Crash modality, updating the resource contexts themselves depends on the operational semantics action ( the last column in Table \ref{table:decomposition}) a resource ambiance is operating on.
%\end{definition}
%The address-space switch updating the cr3 register that holds the nominal value for address-space modality's resource contexts and the resource ambiance's (Hoare Triple indexed with resource context nominal) is an example of updating a resource context in a resource ambiance.
%\paragraph{Composition of Contingent Truth} Another example of weak-nominalization is Ref-Counter Modality (jump modality ($@_{l} \; P$ ~\cite{amalreal2024}) shown in Table \ref{table:decomposition}. The resource context $@_{l} \; P$ is a container of memory nodes (i.e. the resource elements ($\ell \mapsto v$ in Table \ref{table:decomposition}) that are \emph{reachable} by a jump over the shared/duplicable root address $\ell$. In addition to being weakly nominalized due to its agnostic resource elements, it exhibits a good example of how one contingent truth \emph{entails} another. Wagner et.al., also define \emph{reachability} modality --- a monad style modality for specification of non-destructive actions ($\blackdiamond$) ~\cite{amalreal2024} --- to assert the reachability to a certain memory node. Knowing $P$ satisfied under \emph{jump} ~\cite{amalreal2024} modality (\textsf{Ref-Counter Modality} in \ref{table:decomposition}) entails the satisfaction of $P$ under non-destructive reachability modality
%\[ 
%@_{\ell} \;P \vdash  \blackdiamond \; P 
%\]
%\begin{definition}[Resource Interaction]
%A resource interaction rule defines a particular way to interact with a resource context to access resource elements. A resource interaction is defined by a proof rule with the relevant resource context in the specification. 
%\end{definition}
%Although it is almost always the case that when a modality is defined, classical proof rules such as introduction and elimination are also defined and proven. However, an interesting perspective emerges when we realize that, in fact, the proof rule for switching address spaces resembles an introduction (also elimination) rule for adding resource elements into an address-space modality's resource context.
%\Colin{So far I like how this is shaping up as a kind of ``design recipe'' for approaching this style of verification!}


\section{Conclusion}
\label{sec:conclusion}
\todo[inline]{this definitely needs adjusting for both audience and emphasis on this being a survey}
We have argued the essential patterns that help to choose the modalities when working out how to specify different kinds of systems code based on recent successes.
It also captures (a slightly more organized version of) our own thinking in coming up with designs for specifying distributed systems~\cite{gordon2019modal}, virtual memory managers~\cite{kuru2024modalabstractionsvirtualizingmemory}, and in ongoing work using different modalities for different parts of a copy-on-write filesystem (e.g., for assertions true in different snapshots). So not only are the modalities useful
for conducting proofs, the pieces we have identified seem to be an effective way of working out
a specific modality for a specific use-case.

As our main conclusion and future work, we think that these essential patterns in the designs of modal abstractions discussed constitute the fundamentals of \emph{verification patterns} when working out how to specify different kinds of systems code.
\bibliographystyle{splncs04}
\bibliography{software}
\newpage
\appendix

\end{document}
\grid
